{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f35250",
   "metadata": {},
   "source": [
    "# ML-CUP Neural Network with Keras/TensorFlow\n",
    "## Using Mean Euclidean Error (MEE) as Evaluation Metric\n",
    "\n",
    "This notebook solves the ML-CUP dataset using a neural network with Keras and TensorFlow. Since the test set is blind, we split the training data into train/validation/test sets and use MEE (Mean Euclidean Error) as the metric throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e676b4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9696c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a257f",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "# Skip comment lines (lines starting with #)\n",
    "df_train = pd.read_csv('ML-CUP25-TR.csv', comment='#', header=None)\n",
    "\n",
    "print(\"Dataset shape:\", df_train.shape)\n",
    "print(\"\\nFirst rows:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(df_train.info())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_train.describe())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "# Dataset structure: ID, 12 input features, 4 targets\n",
    "# Columns: 0=ID, 1-12=inputs, 13-16=targets\n",
    "print(\"\\nColumn count:\", len(df_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e90aa",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "# Drop ID column (column 0)\n",
    "# Features: columns 1-12 (12 features)\n",
    "# Targets: columns 13-16 (4 targets)\n",
    "\n",
    "X = df_train.iloc[:, 1:13].values  # Features: columns 1-12\n",
    "y = df_train.iloc[:, 13:17].values  # Targets: columns 13-16\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Targets shape:\", y.shape)\n",
    "\n",
    "# Standardize/Normalize the features\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "print(\"\\nFeatures after scaling - mean:\", X_scaled.mean(axis=0)[:5])\n",
    "print(\"Features after scaling - std:\", X_scaled.std(axis=0)[:5])\n",
    "\n",
    "# Also scale the targets for better neural network performance\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "print(\"\\nTargets after scaling - mean:\", y_scaled.mean(axis=0))\n",
    "print(\"Targets after scaling - std:\", y_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b58a38",
   "metadata": {},
   "source": [
    "## 4. Define MEE (Mean Euclidean Error) Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba4ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MEE metric for Keras\n",
    "def mee(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Euclidean Error\n",
    "    Calculates the mean of the Euclidean distance between true and predicted values\n",
    "    For each sample, compute sqrt(sum((y_true - y_pred)^2)) and then average across samples\n",
    "    \"\"\"\n",
    "    euclidean_distances = tf.sqrt(tf.reduce_sum(tf.square(y_true - y_pred), axis=1))\n",
    "    return tf.reduce_mean(euclidean_distances)\n",
    "\n",
    "# Custom loss function based on MEE (for training)\n",
    "def mee_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MEE Loss function for model training\n",
    "    \"\"\"\n",
    "    return mee(y_true, y_pred)\n",
    "\n",
    "print(\"MEE metric defined successfully\")\n",
    "print(\"MEE measures the mean distance between true and predicted points in 4D space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518aa749",
   "metadata": {},
   "source": [
    "## 5. Split Data into Training, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43063778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train (70%), validation (15%), and test (15%)\n",
    "# First split: 70% train, 30% temp (which will be split into val and test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.30, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: split temp into 50% validation, 50% test (15% each of original)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Internal Test set shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "print(\"\\nData split percentages:\")\n",
    "print(f\"Train: {X_train.shape[0]/len(X_scaled)*100:.1f}%\")\n",
    "print(f\"Validation: {X_val.shape[0]/len(X_scaled)*100:.1f}%\")\n",
    "print(f\"Test: {X_test.shape[0]/len(X_scaled)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8cd61",
   "metadata": {},
   "source": [
    "## 6. Build the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2eabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(12,)),  # 12 input features\n",
    "    \n",
    "    # Hidden layers with batch normalization and dropout for regularization\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.1),\n",
    "    \n",
    "    layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    \n",
    "    # Output layer: 4 targets\n",
    "    layers.Dense(4, activation='linear')  # Linear activation for regression\n",
    "])\n",
    "\n",
    "# Compile the model with MEE as loss and metric\n",
    "model.compile(\n",
    "    loss=mee_loss,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[mee]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully\")\n",
    "print(\"\\nModel architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec08383",
   "metadata": {},
   "source": [
    "## 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=300,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total epochs trained: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f234e87",
   "metadata": {},
   "source": [
    "## 8. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b982a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to smooth curves using exponential moving average\n",
    "def exponential_moving_average(data, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Smooth data using exponential moving average\n",
    "    alpha: smoothing factor (0 to 1), higher = more smoothing\n",
    "    \"\"\"\n",
    "    ema = [data[0]]\n",
    "    for i in range(1, len(data)):\n",
    "        ema.append(alpha * data[i] + (1 - alpha) * ema[i-1])\n",
    "    return ema\n",
    "\n",
    "# Plot training history with smoothed curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Smooth the curves\n",
    "loss_smooth = exponential_moving_average(history.history['loss'], alpha=0.15)\n",
    "val_loss_smooth = exponential_moving_average(history.history['val_loss'], alpha=0.15)\n",
    "mee_smooth = exponential_moving_average(history.history['mee'], alpha=0.15)\n",
    "val_mee_smooth = exponential_moving_average(history.history['val_mee'], alpha=0.15)\n",
    "\n",
    "# Plot loss with smoothing\n",
    "axes[0].plot(history.history['loss'], label='Training Loss (Raw)', linewidth=1, alpha=0.3)\n",
    "axes[0].plot(loss_smooth, label='Training Loss (Smoothed)', linewidth=2.5)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss (Raw)', linewidth=1, alpha=0.3)\n",
    "axes[0].plot(val_loss_smooth, label='Validation Loss (Smoothed)', linewidth=2.5)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('MEE Loss', fontsize=12)\n",
    "axes[0].set_title('Model Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot metric (MEE) with smoothing\n",
    "axes[1].plot(history.history['mee'], label='Training MEE (Raw)', linewidth=1, alpha=0.3)\n",
    "axes[1].plot(mee_smooth, label='Training MEE (Smoothed)', linewidth=2.5)\n",
    "axes[1].plot(history.history['val_mee'], label='Validation MEE (Raw)', linewidth=1, alpha=0.3)\n",
    "axes[1].plot(val_mee_smooth, label='Validation MEE (Smoothed)', linewidth=2.5)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MEE Metric', fontsize=12)\n",
    "axes[1].set_title('MEE Metric over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training MEE: {history.history['mee'][-1]:.6f}\")\n",
    "print(f\"Final Validation MEE: {history.history['val_mee'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118b210",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Validation and Internal Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe5d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_loss, val_mee = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Loss (MEE): {val_loss:.6f}\")\n",
    "print(f\"Validation MEE Metric: {val_mee:.6f}\")\n",
    "\n",
    "# Evaluate on internal test set\n",
    "test_loss, test_mee = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERNAL TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss (MEE): {test_loss:.6f}\")\n",
    "print(f\"Test MEE Metric: {test_mee:.6f}\")\n",
    "\n",
    "# Make predictions on test set\n",
    "y_test_pred = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Inverse transform predictions and targets to original scale\n",
    "y_test_pred_original = scaler_y.inverse_transform(y_test_pred)\n",
    "y_test_original = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Calculate MEE in original scale\n",
    "euclidean_distances = np.sqrt(np.sum((y_test_original - y_test_pred_original)**2, axis=1))\n",
    "mee_original = np.mean(euclidean_distances)\n",
    "\n",
    "print(f\"\\nTest MEE in Original Scale: {mee_original:.6f}\")\n",
    "print(f\"Min distance: {euclidean_distances.min():.6f}\")\n",
    "print(f\"Max distance: {euclidean_distances.max():.6f}\")\n",
    "print(f\"Std distance: {euclidean_distances.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c788f",
   "metadata": {},
   "source": [
    "## 10. Generate Predictions for Blind Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the blind test set\n",
    "df_test = pd.read_csv('ML-CUP25-TS.csv', comment='#', header=None)\n",
    "\n",
    "print(\"Blind test set shape:\", df_test.shape)\n",
    "print(\"\\nFirst rows of blind test set:\")\n",
    "print(df_test.head())\n",
    "\n",
    "# Extract features from blind test set\n",
    "# The test set has: ID, 12 input features (no targets)\n",
    "X_blind = df_test.iloc[:, 1:13].values  # Features: columns 1-12\n",
    "\n",
    "# Scale the features using the same scaler fitted on training data\n",
    "X_blind_scaled = scaler_X.transform(X_blind)\n",
    "\n",
    "print(f\"\\nBlind test set features shape: {X_blind_scaled.shape}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_blind_pred_scaled = model.predict(X_blind_scaled, verbose=0)\n",
    "\n",
    "# Inverse transform predictions to original scale\n",
    "y_blind_pred_original = scaler_y.inverse_transform(y_blind_pred_scaled)\n",
    "\n",
    "print(f\"Predictions shape: {y_blind_pred_original.shape}\")\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "print(y_blind_pred_original[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c96215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions in the required format\n",
    "# Format: ID, TARGET_1, TARGET_2, TARGET_3, TARGET_4\n",
    "\n",
    "# Create output dataframe\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': df_test[0].values,  # The first column is the ID (use integer index, not string)\n",
    "    'TARGET_1': y_blind_pred_original[:, 0],\n",
    "    'TARGET_2': y_blind_pred_original[:, 1],\n",
    "    'TARGET_3': y_blind_pred_original[:, 2],\n",
    "    'TARGET_4': y_blind_pred_original[:, 3]\n",
    "})\n",
    "\n",
    "print(\"Output dataframe shape:\", output_df.shape)\n",
    "print(\"\\nFirst 5 rows of output:\")\n",
    "print(output_df.head())\n",
    "\n",
    "# Save to CSV (without index, no header - standard for ML-CUP format)\n",
    "output_filename = 'ML-CUP25-TS-predictions.csv'\n",
    "output_df.to_csv(output_filename, index=False, header=False)\n",
    "\n",
    "print(f\"\\nPredictions saved to: {output_filename}\")\n",
    "\n",
    "# Also save a version with headers for reference\n",
    "output_filename_with_header = 'ML-CUP25-TS-predictions-with-header.csv'\n",
    "output_df.to_csv(output_filename_with_header, index=False, header=True)\n",
    "\n",
    "print(f\"Predictions with header saved to: {output_filename_with_header}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d96b4",
   "metadata": {},
   "source": [
    "## Summary and Results\n",
    "\n",
    "### Key Points:\n",
    "- **Training Data**: Split into 70% training, 15% validation, and 15% internal test\n",
    "- **Loss Function**: Mean Euclidean Error (MEE) - measures Euclidean distance in 4D space\n",
    "- **Data Preprocessing**: Features and targets standardized using StandardScaler\n",
    "- **Model Architecture**: 4 hidden layers (128→64→32→16 neurons) with batch normalization and dropout\n",
    "- **Regularization**: L2 regularization, dropout, and learning rate reduction on plateau\n",
    "\n",
    "### Performance:\n",
    "- See the evaluation metrics printed above\n",
    "- The model was trained with early stopping to prevent overfitting\n",
    "- Validation MEE is used to monitor generalization\n",
    "\n",
    "### Predictions:\n",
    "- Blind test set predictions saved in the required format\n",
    "- Predictions are in the original (unscaled) target space"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
