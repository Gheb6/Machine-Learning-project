# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B-UJDjiDwAk4R6do_PYHjjD9_oO1gPkO
"""

# ============================================================
# DATASET STUDY FILE — NO MODELS, NO TRAINING, NO BULLSHIT
# Purpose: Understand data geometry, structure, targets, patterns
# Supports: ID | INPUTS | MULTIPLE TARGETS
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.feature_selection import mutual_info_regression

sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (8, 5)

# =========================
# CONFIG — CHANGE ONLY THIS
# =========================

DATA_PATH = "/content/sample_data/ML-CUP25-TS.csv"
N_TARGETS = 4          # number of target columns at the end
HAS_ID = True          # first column is ID

# ======================
# LOAD DATA (NO HEADER)
# ======================

df = pd.read_csv(
    "/content/sample_data/ML-CUP25-TS.csv",
    header=None,
    comment="#",
    sep=","
)

print("\n=== RAW DATA SHAPE ===")
print(df.shape)
display(df.head())

# ======================
# SPLIT STRUCTURE
# ======================

start_inputs = 1 if HAS_ID else 0
end_inputs = df.shape[1] - N_TARGETS

X = df.iloc[:, start_inputs:end_inputs]
Y = df.iloc[:, -N_TARGETS:]

print("\n=== STRUCTURE ===")
print("Inputs shape :", X.shape)
print("Targets shape:", Y.shape)

# ======================
# BASIC SANITY CHECKS
# ======================

print("\n=== MISSING VALUES (%) ===")
display(df.isna().mean() * 100)

print("\n=== DUPLICATED ROWS ===")
print(df.duplicated().sum())

# ======================
# INPUT DISTRIBUTIONS
# ======================

print("\n=== INPUT DISTRIBUTIONS ===")
for col in X.columns:
    plt.figure()
    sns.histplot(X[col], kde=True)
    plt.title(f"Input {col} | skew={X[col].skew():.2f}")
    plt.show()

# ======================
# INPUT OUTLIERS
# ======================

print("\n=== INPUT OUTLIER FRACTION (|z| > 3) ===")
outliers = {}
for col in X.columns:
    z = np.abs(stats.zscore(X[col]))
    outliers[col] = (z > 3).mean()

display(pd.Series(outliers).sort_values(ascending=False))

# ======================
# INPUT CORRELATION
# ======================

plt.figure(figsize=(10, 8))
sns.heatmap(X.corr(), cmap="coolwarm", center=0)
plt.title("Input Correlation Matrix")
plt.show()

# ======================
# TARGET DISTRIBUTIONS
# ======================

print("\n=== TARGET DISTRIBUTIONS ===")
for col in Y.columns:
    plt.figure()
    sns.histplot(Y[col], kde=True)
    plt.title(f"Target {col}")
    plt.show()

# ======================
# TARGET CORRELATION
# ======================

plt.figure(figsize=(6, 5))
sns.heatmap(Y.corr(), annot=True, cmap="coolwarm", center=0)
plt.title("Target Correlation Matrix")
plt.show()

# ======================
# INPUT → TARGET DEPENDENCE (NONLINEAR)
# ======================

print("\n=== MUTUAL INFORMATION (Inputs vs Targets) ===")

mi_table = {}

for t in Y.columns:
    mi = mutual_info_regression(X, Y[t], random_state=42)
    mi_table[t] = pd.Series(mi, index=X.columns)

mi_df = pd.DataFrame(mi_table)

plt.figure(figsize=(12, 4))
mi_df.plot(kind="bar")
plt.title("Mutual Information: Inputs → Each Target")
plt.show()

# ======================
# INPUT SCALING
# ======================

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ======================
# PCA — INTRINSIC DIMENSION
# ======================

pca = PCA()
Xp = pca.fit_transform(X_scaled)

plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker="o")
plt.xlabel("Components")
plt.ylabel("Cumulative explained variance")
plt.title("Input Intrinsic Dimensionality (PCA)")
plt.show()

dims_90 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9) + 1
print("Components for 90% variance:", dims_90)

# ======================
# PCA 2D PROJECTION
# ======================

plt.figure()
plt.scatter(Xp[:, 0], Xp[:, 1], s=15)
plt.title("Inputs — PCA 2D Projection")
plt.show()

# ======================
# t-SNE — NONLINEAR MANIFOLD
# ======================

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
Xt = tsne.fit_transform(X_scaled)

plt.figure()
plt.scatter(Xt[:, 0], Xt[:, 1], s=15)
plt.title("Inputs — t-SNE Manifold")
plt.show()

# ======================
# FEATURE REDUNDANCY
# ======================

print("\n=== HIGHLY CORRELATED INPUT PAIRS (|corr| > 0.85) ===")
corr = X.corr()
pairs = []

for i in range(len(X.columns)):
    for j in range(i + 1, len(X.columns)):
        if abs(corr.iloc[i, j]) > 0.85:
            pairs.append((X.columns[i], X.columns[j], corr.iloc[i, j]))

for p in pairs:
    print(p)

# ======================
# FINAL DATASET DIAGNOSIS
# ======================

print("\n=== FINAL DIAGNOSIS ===")
print("Samples              :", X.shape[0])
print("Input dimensions     :", X.shape[1])
print("Number of targets    :", Y.shape[1])
print("Strong input redundancy:", len(pairs) > 0)
print("Low intrinsic dimension:", dims_90 < X.shape[1] // 2)
print("Target correlations  :")
display(Y.corr())