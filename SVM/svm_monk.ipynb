{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Random State Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Generate random state for reproducibility within this session\n",
    "RANDOM_STATE = np.random.randint(0, 10000)\n",
    "print(f\"Random State for this session: {RANDOM_STATE}\")\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8625315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Loading Function\n",
    "def load_monk_data(filepath):\n",
    "    \"\"\"\n",
    "    Load MONK dataset from file.\n",
    "    \n",
    "    The MONK datasets have a specific format:\n",
    "    - First column: class label (0 or 1)\n",
    "    - Next 6 columns: attributes a1-a6 (categorical values)\n",
    "    - Additional columns are ignored (sample ID, etc.)\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the MONK dataset file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) where X is the feature matrix (n_samples, 6) and \n",
    "               y is the label vector (n_samples,)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip empty lines\n",
    "            if line.strip():\n",
    "                values = line.strip().split()\n",
    "                # First value is the class (0 or 1), next 6 are attributes\n",
    "                data.append([int(v) for v in values[:7]])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['class', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6'])\n",
    "    X = df[['a1', 'a2', 'a3', 'a4', 'a5', 'a6']].values\n",
    "    y = df['class'].values\n",
    "    return X, y\n",
    "\n",
    "print(\"✓ load_monk_data() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Preprocessing Function\n",
    "def one_hot_encode_features(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    One-hot encode categorical features.\n",
    "    \n",
    "    The MONK datasets contain categorical features with no ordinal relationship.\n",
    "    One-hot encoding converts each categorical value into a binary feature,\n",
    "    which is important for distance-based and linear models to avoid treating\n",
    "    categorical values as having magnitude.\n",
    "    \n",
    "    Example: a1 ∈ {1, 2, 3} becomes three binary features: a1_2, a1_3 \n",
    "    (a1_1 is dropped to avoid multicollinearity)\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.ndarray): Training feature matrix\n",
    "        X_val (np.ndarray): Validation feature matrix\n",
    "        X_test (np.ndarray): Test feature matrix\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train_encoded, X_val_encoded, X_test_encoded, encoder)\n",
    "               where encoded matrices have binary features and encoder can be\n",
    "               reused for future transformations\n",
    "    \"\"\"\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    X_train_encoded = encoder.fit_transform(X_train)\n",
    "    X_val_encoded = encoder.transform(X_val)\n",
    "    X_test_encoded = encoder.transform(X_test)\n",
    "    return X_train_encoded, X_val_encoded, X_test_encoded, encoder\n",
    "\n",
    "print(\"✓ one_hot_encode_features() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8687d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Plotting Functions\n",
    "def plot_confusion_matrix(y_true, y_pred, dataset_number):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix as PDF.\n",
    "    \n",
    "    A confusion matrix shows the counts of true positives, true negatives,\n",
    "    false positives, and false negatives, providing insight into the types\n",
    "    of errors the model makes.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): True labels\n",
    "        y_pred (np.ndarray): Predicted labels\n",
    "        dataset_number (int): MONK dataset number (1, 2, or 3)\n",
    "        \n",
    "    Side Effects:\n",
    "        Saves a PDF file named 'confusion_matrix_monk{dataset_number}.pdf'\n",
    "        in the current directory\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    \n",
    "    plt.title(f'Confusion Matrix - MONK-{dataset_number} Dataset (Test Set)', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'confusion_matrix_monk{dataset_number}.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Confusion matrix saved as 'confusion_matrix_monk{dataset_number}.pdf'\")\n",
    "\n",
    "\n",
    "def plot_hyperparameter_heatmap(all_results, dataset_number):\n",
    "    \"\"\"\n",
    "    Plot heatmap of hyperparameter performance for RBF kernel.\n",
    "    \n",
    "    Shows how different C and gamma combinations affect validation accuracy,\n",
    "    helping to visualize the hyperparameter search space and identify optimal regions.\n",
    "    \n",
    "    Args:\n",
    "        all_results (list): List of dictionaries containing params and val_score\n",
    "        dataset_number (int): MONK dataset number (1, 2, or 3)\n",
    "        \n",
    "    Side Effects:\n",
    "        Saves a PDF file named 'hyperparameter_heatmap_monk{dataset_number}.pdf'\n",
    "    \"\"\"\n",
    "    # Filter for RBF kernel results with numeric gamma\n",
    "    rbf_results = [r for r in all_results if r['params'].get('kernel') == 'rbf' \n",
    "                   and isinstance(r['params'].get('gamma'), (int, float))]\n",
    "    \n",
    "    if not rbf_results:\n",
    "        print(\"No RBF results with numeric gamma found for heatmap\")\n",
    "        return\n",
    "    \n",
    "    # Create pivot table\n",
    "    data = []\n",
    "    for result in rbf_results:\n",
    "        data.append({\n",
    "            'C': result['params']['C'],\n",
    "            'gamma': result['params']['gamma'],\n",
    "            'accuracy': result['val_score']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    pivot = df.pivot_table(values='accuracy', index='gamma', columns='C', aggfunc='mean')\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': 'Validation Accuracy'})\n",
    "    plt.title(f'RBF Kernel: C vs Gamma Performance - MONK-{dataset_number}', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('C (Regularization Parameter)', fontsize=12)\n",
    "    plt.ylabel('Gamma (Kernel Coefficient)', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'hyperparameter_heatmap_monk{dataset_number}.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Hyperparameter heatmap saved as 'hyperparameter_heatmap_monk{dataset_number}.pdf'\")\n",
    "\n",
    "\n",
    "def plot_kernel_comparison(all_results, dataset_number):\n",
    "    \"\"\"\n",
    "    Plot bar chart comparing performance of different kernel types.\n",
    "    \n",
    "    Shows the best validation accuracy achieved by each kernel type,\n",
    "    helping to understand which kernel works best for the dataset.\n",
    "    \n",
    "    Args:\n",
    "        all_results (list): List of dictionaries containing params and val_score\n",
    "        dataset_number (int): MONK dataset number (1, 2, or 3)\n",
    "        \n",
    "    Side Effects:\n",
    "        Saves a PDF file named 'kernel_comparison_monk{dataset_number}.pdf'\n",
    "    \"\"\"\n",
    "    # Group by kernel and find best score\n",
    "    kernel_best = {}\n",
    "    for result in all_results:\n",
    "        kernel = result['params']['kernel']\n",
    "        score = result['val_score']\n",
    "        if kernel not in kernel_best or score > kernel_best[kernel]:\n",
    "            kernel_best[kernel] = score\n",
    "    \n",
    "    # Sort by score\n",
    "    kernels = sorted(kernel_best.items(), key=lambda x: x[1], reverse=True)\n",
    "    kernel_names = [k[0] for k in kernels]\n",
    "    scores = [k[1] for k in kernels]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(kernel_names, scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'][:len(kernels)])\n",
    "    plt.xlabel('Kernel Type', fontsize=12)\n",
    "    plt.ylabel('Best Validation Accuracy', fontsize=12)\n",
    "    plt.title(f'Kernel Comparison - MONK-{dataset_number} Dataset', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'kernel_comparison_monk{dataset_number}.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Kernel comparison saved as 'kernel_comparison_monk{dataset_number}.pdf'\")\n",
    "\n",
    "\n",
    "def plot_c_parameter_analysis(all_results, dataset_number):\n",
    "    \"\"\"\n",
    "    Plot line chart showing how C parameter affects performance for each kernel.\n",
    "    \n",
    "    Helps visualize the effect of regularization strength on model performance.\n",
    "    \n",
    "    Args:\n",
    "        all_results (list): List of dictionaries containing params and val_score\n",
    "        dataset_number (int): MONK dataset number (1, 2, or 3)\n",
    "        \n",
    "    Side Effects:\n",
    "        Saves a PDF file named 'c_parameter_analysis_monk{dataset_number}.pdf'\n",
    "    \"\"\"\n",
    "    # Group with kernel and C\n",
    "    kernel_c_scores = {}\n",
    "    for result in all_results:\n",
    "        kernel = result['params']['kernel']\n",
    "        c = result['params']['C']\n",
    "        score = result['val_score']\n",
    "        \n",
    "        if kernel not in kernel_c_scores:\n",
    "            kernel_c_scores[kernel] = {}\n",
    "        if c not in kernel_c_scores[kernel]:\n",
    "            kernel_c_scores[kernel][c] = []\n",
    "        kernel_c_scores[kernel][c].append(score)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for kernel, c_scores in kernel_c_scores.items():\n",
    "        c_values = sorted(c_scores.keys())\n",
    "        avg_scores = [np.mean(c_scores[c]) for c in c_values]\n",
    "        plt.plot(c_values, avg_scores, marker='o', label=kernel, linewidth=2, markersize=8)\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('C (Regularization Parameter)', fontsize=12)\n",
    "    plt.ylabel('Average Validation Accuracy', fontsize=12)\n",
    "    plt.title(f'C Parameter Impact on Performance - MONK-{dataset_number} Dataset', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'c_parameter_analysis_monk{dataset_number}.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"C parameter analysis saved as 'c_parameter_analysis_monk{dataset_number}.pdf'\")\n",
    "\n",
    "print(\"✓ Plotting functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Dataset-Specific Parameter Grid\n",
    "def get_param_grid(dataset_number):\n",
    "    \"\"\"\n",
    "    Get dataset-specific hyperparameter grid for grid search.\n",
    "    \n",
    "    Each MONK dataset has different characteristics that benefit from\n",
    "    different SVM configurations:\n",
    "    \n",
    "    MONK-1: Linearly separable problem\n",
    "        - Rule: (a1 == a2) OR (a5 == 1)\n",
    "        - Best: Linear or RBF kernels with moderate C values\n",
    "        \n",
    "    MONK-2: Complex XOR-like problem\n",
    "        - Rule: exactly two of {a1==1, a2==1, a3==1, a4==1, a5==1, a6==1}\n",
    "        - Best: Polynomial or RBF kernels with higher complexity\n",
    "        \n",
    "    MONK-3: Noisy version of MONK-1\n",
    "        - Rule: (a5 == 3 AND a4 == 1) OR (a5 != 4 AND a2 != 3) + 5% noise\n",
    "        - Best: Regularized models (lower C) to handle noise\n",
    "    \n",
    "    Args:\n",
    "        dataset_number (int): MONK dataset number (1, 2, or 3)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of parameter dictionaries for GridSearchCV.\n",
    "              Each dictionary specifies a kernel type and its associated parameters.\n",
    "    \"\"\"\n",
    "    if dataset_number == 1:\n",
    "        # MONK-1: Linearly separable, try RBF and Poly with various settings\n",
    "        return [\n",
    "            {'kernel': ['linear'], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "            {'kernel': ['rbf'], 'C': [0.1, 1, 10, 100, 1000], 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]},\n",
    "            {'kernel': ['poly'], 'C': [0.1, 1, 10, 100], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']},\n",
    "        ]\n",
    "    elif dataset_number == 2:\n",
    "        # MONK-2: Complex XOR-like, needs polynomial or RBF\n",
    "        return [\n",
    "            {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]},\n",
    "            {'kernel': ['rbf'], 'C': [1, 10, 100, 1000], 'gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]},\n",
    "            {'kernel': ['poly'], 'C': [1, 10, 100, 1000], 'degree': [2, 3, 4, 5], 'gamma': ['scale', 'auto', 0.1, 1], 'coef0': [0, 1, 10]},\n",
    "        ]\n",
    "    else:  # MONK-3\n",
    "        # MONK-3: Noisy data, linear or mild non-linear\n",
    "        return [\n",
    "            {'kernel': ['linear'], 'C': [0.01, 0.1, 1, 10, 100]},\n",
    "            {'kernel': ['rbf'], 'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto', 0.01, 0.1, 1]},\n",
    "            {'kernel': ['poly'], 'C': [0.1, 1, 10, 100], 'degree': [2, 3], 'gamma': ['scale', 'auto']},\n",
    "        ]\n",
    "\n",
    "print(\"✓ get_param_grid() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Manual Grid Search Function\n",
    "def manual_grid_search(X_train, y_train, X_val, y_val, param_grid, random_state):\n",
    "    \"\"\"\n",
    "    Perform manual grid search using hold-out validation (80/20 split).\n",
    "    \n",
    "    This function evaluates all hyperparameter combinations on the\n",
    "    validation set and returns the best model and parameters.\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.ndarray): Training features\n",
    "        y_train (np.ndarray): Training labels\n",
    "        X_val (np.ndarray): Validation features\n",
    "        y_val (np.ndarray): Validation labels\n",
    "        param_grid (list): List of parameter dictionaries\n",
    "        random_state (int): Random state for SVM\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, best_params, best_score, all_results) where:\n",
    "            - best_model: SVM model with best parameters\n",
    "            - best_params: Dictionary of best hyperparameters\n",
    "            - best_score: Best validation accuracy\n",
    "            - all_results: List of all results for analysis\n",
    "    \"\"\"\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    all_results = []\n",
    "    \n",
    "    # Flatten the list of parameter grids into individual parameter combinations\n",
    "    for param_dict in param_grid:\n",
    "        for params in ParameterGrid(param_dict):\n",
    "            # Train model with current parameters\n",
    "            model = SVC(**params, random_state=random_state)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_score = model.score(X_val, y_val)\n",
    "            \n",
    "            # Store results\n",
    "            all_results.append({\n",
    "                'params': params,\n",
    "                'val_score': val_score\n",
    "            })\n",
    "            \n",
    "            # Update best model if necessary\n",
    "            if val_score > best_score:\n",
    "                best_score = val_score\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "    \n",
    "    return best_model, best_params, best_score, all_results\n",
    "\n",
    "print(\"✓ manual_grid_search() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec629c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main Training and Evaluation Function\n",
    "def train_and_evaluate_svm(dataset_number, random_state):\n",
    "    \"\"\"\n",
    "    Train and evaluate SVM on a specific MONK dataset.\n",
    "    \n",
    "    Training Process:\n",
    "    1. Load training and test data\n",
    "    2. Split training data into 80% train and 20% validation (hold-out)\n",
    "    3. One-hot encode categorical features\n",
    "    4. Standardize features (zero mean, unit variance)\n",
    "    5. Perform grid search using hold-out validation to find best hyperparameters\n",
    "    6. Evaluate best model on validation set\n",
    "    7. Retrain on full training set (train + validation) with best hyperparameters\n",
    "    8. Evaluate final model on test set\n",
    "    9. Generate and save confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        dataset_number (int): MONK dataset number (1, 2, or 3)\n",
    "        random_state (int): Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (svm_final, test_acc, best_params, all_results, metrics) where:\n",
    "            - svm_final: Trained SVM model on full training set\n",
    "            - test_acc: Test set accuracy (float)\n",
    "            - best_params: Dictionary of best hyperparameters found\n",
    "            - all_results: List of all grid search results\n",
    "            - metrics: Dictionary with train_acc, val_acc, test_acc\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MONK-{dataset_number} Dataset\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load training and test data\n",
    "    train_path = f'../monk_dataset/monks-{dataset_number}.train'\n",
    "    test_path = f'../monk_dataset/monks-{dataset_number}.test'\n",
    "    \n",
    "    X_train_full, y_train_full = load_monk_data(train_path)\n",
    "    X_test, y_test = load_monk_data(test_path)\n",
    "    \n",
    "    # Split training data: 80% train, 20% validation (hold-out)\n",
    "    # Stratified split ensures class balance is maintained in both splits\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.2, random_state=random_state, stratify=y_train_full\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData sizes:\")\n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    # This is crucial for MONK datasets as features are categorical with no ordinal relationship\n",
    "    print(\"\\nApplying one-hot encoding...\")\n",
    "    X_train_encoded, X_val_encoded, X_test_temp, encoder = one_hot_encode_features(X_train, X_val, X_test)\n",
    "    print(f\"Feature dimension after encoding: {X_train_encoded.shape[1]}\")\n",
    "    \n",
    "    # Standardize features (mean=0, std=1)\n",
    "    # While not strictly necessary for all kernels, standardization helps with:\n",
    "    # - Numerical stability\n",
    "    # - Faster convergence\n",
    "    # - Fair comparison of C values across datasets\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "    X_val_scaled = scaler.transform(X_val_encoded)\n",
    "    \n",
    "    # Get dataset-specific parameter grid\n",
    "    param_grid = get_param_grid(dataset_number)\n",
    "    \n",
    "    # Perform manual grid search using hold-out validation (80/20)\n",
    "    print(\"\\n--- Phase 1: Grid Search for Hyperparameter Tuning ---\")\n",
    "    print(\"Using hold-out validation (80% train, 20% validation)...\")\n",
    "    \n",
    "    best_model, best_params, best_score, all_results = manual_grid_search(\n",
    "        X_train_scaled, y_train, X_val_scaled, y_val, param_grid, random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBest parameters found: {best_params}\")\n",
    "    print(f\"Best validation accuracy: {best_score:.4f}\")\n",
    "    \n",
    "    # Evaluate best model on training and validation sets\n",
    "    y_train_pred = best_model.predict(X_train_scaled)\n",
    "    y_val_pred = best_model.predict(X_val_scaled)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Results with Best Model:\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    \n",
    "    # Display top 10 parameter combinations to understand what works well\n",
    "    print(\"\\nTop 10 parameter combinations:\")\n",
    "    sorted_results = sorted(all_results, key=lambda x: x['val_score'], reverse=True)\n",
    "    for rank, result in enumerate(sorted_results[:10], 1):\n",
    "        params_str = ', '.join([f\"{k}={v}\" for k, v in result['params'].items()])\n",
    "        print(f\"  Rank {rank}: {params_str}, score={result['val_score']:.4f}\")\n",
    "    \n",
    "    # Retrain on full training data (train + validation) with best parameters\n",
    "    # This maximizes the use of available training data for the final model\n",
    "    print(\"\\n--- Phase 2: Final Model Training on Full Training Set ---\")\n",
    "    \n",
    "    # One-hot encode full training and test data\n",
    "    # Important: Use a new encoder fitted on the full training set\n",
    "    encoder_final = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    X_train_full_encoded = encoder_final.fit_transform(X_train_full)\n",
    "    X_test_encoded = encoder_final.transform(X_test)\n",
    "    \n",
    "    # Standardize using full training set statistics\n",
    "    scaler_final = StandardScaler()\n",
    "    X_train_full_scaled = scaler_final.fit_transform(X_train_full_encoded)\n",
    "    X_test_scaled = scaler_final.transform(X_test_encoded)\n",
    "    \n",
    "    # Train final model on all training data with best parameters\n",
    "    svm_final = SVC(**best_params, random_state=random_state)\n",
    "    svm_final.fit(X_train_full_scaled, y_train_full)\n",
    "    \n",
    "    # Final predictions\n",
    "    y_train_full_pred = svm_final.predict(X_train_full_scaled)\n",
    "    y_test_pred = svm_final.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate final accuracies\n",
    "    train_full_acc = accuracy_score(y_train_full, y_train_full_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Full Training Accuracy: {train_full_acc:.4f} ({train_full_acc*100:.2f}%)\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    \n",
    "    # Classification report provides precision, recall, and F1-score per class\n",
    "    print(f\"\\nTest Set Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    # Confusion matrix shows distribution of predictions\n",
    "    print(f\"Test Set Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    \n",
    "    # Generate all visualizations\n",
    "    print(\"\\n--- Generating Visualizations ---\")\n",
    "    plot_confusion_matrix(y_test, y_test_pred, dataset_number)\n",
    "    plot_hyperparameter_heatmap(all_results, dataset_number)\n",
    "    plot_kernel_comparison(all_results, dataset_number)\n",
    "    plot_c_parameter_analysis(all_results, dataset_number)\n",
    "    \n",
    "    # Store metrics for saving to file\n",
    "    metrics = {\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'train_full_acc': train_full_acc\n",
    "    }\n",
    "    \n",
    "    return svm_final, test_acc, best_params, all_results, metrics\n",
    "\n",
    "print(\"✓ train_and_evaluate_svm() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Overall Comparison Plotting Function\n",
    "def plot_overall_comparison(all_dataset_results):\n",
    "    \"\"\"\n",
    "    Plot comparison of all three MONK datasets performance.\n",
    "    \n",
    "    Shows train vs test accuracy for each dataset to identify overfitting.\n",
    "    \n",
    "    Args:\n",
    "        all_dataset_results (dict): Dictionary with results for all datasets\n",
    "        \n",
    "    Side Effects:\n",
    "        Saves a PDF file named 'monk_overall_comparison.pdf'\n",
    "    \"\"\"\n",
    "    datasets = ['MONK-1', 'MONK-2', 'MONK-3']\n",
    "    train_accs = [all_dataset_results[d]['train_acc'] for d in datasets]\n",
    "    test_accs = [all_dataset_results[d]['test_acc'] for d in datasets]\n",
    "    \n",
    "    x = np.arange(len(datasets))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, train_accs, width, label='Training Accuracy', color='#2ca02c')\n",
    "    bars2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy', color='#1f77b4')\n",
    "    \n",
    "    ax.set_xlabel('Dataset', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('SVM Performance Comparison Across MONK Datasets', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(datasets)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('monk_overall_comparison.pdf', format='pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Overall comparison saved as 'monk_overall_comparison.pdf'\")\n",
    "\n",
    "print(\"✓ plot_overall_comparison() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Execution - Train and Evaluate All MONK Datasets\n",
    "\"\"\"\n",
    "Main execution block.\n",
    "\n",
    "Trains and evaluates SVM classifiers on all three MONK datasets,\n",
    "then prints a summary of results including test accuracies and\n",
    "best hyperparameters for each dataset.\n",
    "Saves train, validation, and test accuracies to a text file.\n",
    "\"\"\"\n",
    "\n",
    "# Train and evaluate on all three MONK datasets\n",
    "results = {}\n",
    "best_params_summary = {}\n",
    "all_dataset_results = {}\n",
    "all_metrics = {}\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    model, test_acc, best_params, grid_results, metrics = train_and_evaluate_svm(i, RANDOM_STATE)\n",
    "    results[f'MONK-{i}'] = test_acc\n",
    "    best_params_summary[f'MONK-{i}'] = best_params\n",
    "    all_metrics[f'MONK-{i}'] = metrics\n",
    "    \n",
    "    # Store for comparison plot\n",
    "    all_dataset_results[f'MONK-{i}'] = {\n",
    "        'train_acc': metrics['train_acc'],\n",
    "        'test_acc': test_acc\n",
    "    }\n",
    "\n",
    "# Create overall comparison plot\n",
    "print(\"\\n--- Generating Overall Comparison ---\")\n",
    "plot_overall_comparison(all_dataset_results)\n",
    "\n",
    "# Save results to text file\n",
    "results_filename = f'svm_monk_results_rs{RANDOM_STATE}.txt'\n",
    "with open(results_filename, 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"SVM MONK Dataset Results\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Random State: {RANDOM_STATE}\\n\")\n",
    "    f.write(f\"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    for dataset in ['MONK-1', 'MONK-2', 'MONK-3']:\n",
    "        f.write(f\"{dataset}\\n\")\n",
    "        f.write(\"-\"*40 + \"\\n\")\n",
    "        f.write(f\"Training Accuracy:   {all_metrics[dataset]['train_acc']:.4f} ({all_metrics[dataset]['train_acc']*100:.2f}%)\\n\")\n",
    "        f.write(f\"Validation Accuracy: {all_metrics[dataset]['val_acc']:.4f} ({all_metrics[dataset]['val_acc']*100:.2f}%)\\n\")\n",
    "        f.write(f\"Test Accuracy:       {all_metrics[dataset]['test_acc']:.4f} ({all_metrics[dataset]['test_acc']*100:.2f}%)\\n\")\n",
    "        f.write(f\"Full Train Accuracy: {all_metrics[dataset]['train_full_acc']:.4f} ({all_metrics[dataset]['train_full_acc']*100:.2f}%)\\n\")\n",
    "        f.write(f\"Best Parameters:     {best_params_summary[dataset]}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"Summary Table\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"{'Dataset':<10} {'Train':<12} {'Validation':<12} {'Test':<12}\\n\")\n",
    "    f.write(\"-\"*46 + \"\\n\")\n",
    "    for dataset in ['MONK-1', 'MONK-2', 'MONK-3']:\n",
    "        train = all_metrics[dataset]['train_acc']\n",
    "        val = all_metrics[dataset]['val_acc']\n",
    "        test = all_metrics[dataset]['test_acc']\n",
    "        f.write(f\"{dataset:<10} {train:.4f}       {val:.4f}       {test:.4f}\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"\\n✓ Results saved to '{results_filename}'\")\n",
    "\n",
    "# Print summary of all results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Summary of Results\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Random State used: {RANDOM_STATE}\\n\")\n",
    "for dataset in ['MONK-1', 'MONK-2', 'MONK-3']:\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    print(f\"  Training Accuracy:   {all_metrics[dataset]['train_acc']:.4f} ({all_metrics[dataset]['train_acc']*100:.2f}%)\")\n",
    "    print(f\"  Validation Accuracy: {all_metrics[dataset]['val_acc']:.4f} ({all_metrics[dataset]['val_acc']*100:.2f}%)\")\n",
    "    print(f\"  Test Accuracy:       {all_metrics[dataset]['test_acc']:.4f} ({all_metrics[dataset]['test_acc']*100:.2f}%)\")\n",
    "    print(f\"  Best Parameters: {best_params_summary[dataset]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ All experiments completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a03999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Download All Generated Files (for Google Colab)\n",
    "\"\"\"\n",
    "Download all generated PDF files as a ZIP archive.\n",
    "This cell is useful when running the notebook on Google Colab.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def download_all_results():\n",
    "    \"\"\"\n",
    "    Create a ZIP file containing all generated PDFs and download it.\n",
    "    Works in Google Colab environment.\n",
    "    \"\"\"\n",
    "    # List of all expected output files\n",
    "    output_files = [\n",
    "        'confusion_matrix_monk1.pdf',\n",
    "        'confusion_matrix_monk2.pdf',\n",
    "        'confusion_matrix_monk3.pdf',\n",
    "        'hyperparameter_heatmap_monk1.pdf',\n",
    "        'hyperparameter_heatmap_monk2.pdf',\n",
    "        'hyperparameter_heatmap_monk3.pdf',\n",
    "        'kernel_comparison_monk1.pdf',\n",
    "        'kernel_comparison_monk2.pdf',\n",
    "        'kernel_comparison_monk3.pdf',\n",
    "        'c_parameter_analysis_monk1.pdf',\n",
    "        'c_parameter_analysis_monk2.pdf',\n",
    "        'c_parameter_analysis_monk3.pdf',\n",
    "        'monk_overall_comparison.pdf'\n",
    "    ]\n",
    "    \n",
    "    # Create ZIP file\n",
    "    zip_filename = f'svm_monk_results_rs{RANDOM_STATE}.zip'\n",
    "    \n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        files_added = 0\n",
    "        for file in output_files:\n",
    "            if os.path.exists(file):\n",
    "                zipf.write(file)\n",
    "                files_added += 1\n",
    "                print(f\"✓ Added: {file}\")\n",
    "            else:\n",
    "                print(f\"✗ Missing: {file}\")\n",
    "        \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ZIP Archive created: {zip_filename}\")\n",
    "    print(f\"Files included: {files_added}/{len(output_files)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Download in Colab environment\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(zip_filename)\n",
    "        print(f\"✓ Download started: {zip_filename}\")\n",
    "    except ImportError:\n",
    "        print(f\"ℹ Not in Colab environment. ZIP file saved locally: {zip_filename}\")\n",
    "        print(f\"ℹ File location: {os.path.abspath(zip_filename)}\")\n",
    "\n",
    "# Uncomment the line below to download all results after execution\n",
    "# download_all_results()\n",
    "\n",
    "print(\"✓ download_all_results() defined\")\n",
    "print(\"ℹ Run 'download_all_results()' to create ZIP and download all PDFs\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
