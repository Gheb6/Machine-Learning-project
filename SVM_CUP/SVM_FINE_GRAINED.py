# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EvtK6DWPnqpA5gZLYyRpj4patLC2nm7_
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from math import sqrt, log

from sklearn.model_selection import KFold, GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet #VECCHI, MA MESSI QUA PER POSSIBILI FUTURI CONFRONTI
from sklearn.multioutput import MultiOutputRegressor # IMPORTANTE ! SERVE PER FAR SI CHE SVM FUNZIONI SU VARI INPUT (FA SVM MULTIPLE)
from sklearn.svm import SVR, LinearSVR # SVM PER REGRESSIONE
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, pairwise_distances, make_scorer
from sklearn.base import clone
from prettytable import PrettyTable


# Custom scorer per MEE (Mean Euclidean Error)
def mean_euclidean_error(y_true, y_pred):
    """Calcola la Mean Euclidean Error tra vettori multidimensionali"""
    return np.mean(np.linalg.norm(y_true - y_pred, axis=1))
# Utilizziamo make_scorer per trasformarlo in uno scorer per GridSearch
# Scorer negativo per GridSearchCV (che massimizza, quindi usiamo il negativo)
mee_scorer = make_scorer(mean_euclidean_error, greater_is_better=False)

"""**1. Caricamento dei Dati**
- Prendo i path (file devono essere caricati in "sample_data")
- TS è il BLIND_TEST, non viene toccato fino alla fine;
- Split Iniziale tramite Hold-Out (80-20) solo sul TR;
"""

TR_PATH = "/content/sample_data/ML-CUP25-TR.csv"
BLIND_TEST_PATH  = "/content/sample_data/ML-CUP25-TS.csv"

# Caricamento (rimuovo anche i commenti essendo presenti nel file dataset)
tr_full = pd.read_csv(TR_PATH, header=None, comment="#")
blind_full = pd.read_csv(BLIND_TEST_PATH , header=None, comment="#")

# Dati completi
X_full = tr_full.iloc[:, 1:13].values # Input X: colonne 1-12 (13 esclusa)
Y_full = tr_full.iloc[:, 13:17].values # Target Y: colonne 13-16 (17 esclusa)
X_blind_test = blind_full.iloc[:, 1:13].values  # BLIND-TEST: serve solo alla fine

# -------------------------------
# HOLD-OUT SPLIT: "DEVELOPMENT SET (X_dev U Y_dev)" vs "INTERNAL TEST SET (X_test_int U Y_test_int)"
# -------------------------------
# 80% Development Set (per training + CV)
# 20% Internal Test Set (per valutazione finale durante sviluppo, BLOCCATO fino alla fine)
X_dev, X_test_int, Y_dev, Y_test_int = train_test_split(
    X_full, Y_full, test_size=0.2, random_state=42
)

print(f"Data Split:")
print(f"-> Development Set (GridSearch/CV): {X_dev.shape[0]} samples")
print(f"-> Internal Test Set (Hold-out):    {X_test_int.shape[0]} samples")
print(f"\n⚠️ Il 20% Internal Test Set è BLOCCATO fino alla valutazione finale")

"""**2. Creazione delle Pipelines**
- Ogni tipo di Modello ha la sua Pipeline;
- Definisce "Scaler e PCA" di ogni modello;
"""

pipelines = {
    "SVR_rbf": Pipeline([
        ("scaler", StandardScaler()),
        ("pca", "passthrough"),   # tuned via GridSearch
        ("model", MultiOutputRegressor(
            SVR(kernel="rbf")
        ))
    ])
}

"""**3. Creazione della griglia dei Parametri da Controllare**
- Definisce quali e quanti parametri sono da testare nella GridSearch;
- *Numero componenti PCA:* Serve a ridurre la dimensionalità dei dati;
- *Numero C:* per la regolarizzazione dei modelli SVR controlla il compromesso tra complessità del modello ed errore di training;
- *Parametro epsilon:* definisce l’ampiezza della zona di tolleranza dell’errore nel modello SVR (ε-insensitive loss);
- *Parametro gamma:* solo per SVR con kernel RBF, controlla l’influenza dei singoli campioni (valori bassi = modello più liscio, valori alti = maggiore complessità).
"""

param_grids = {

    "SVR_rbf": [
        {
            "pca": [PCA()],
            "pca__n_components": [4, 5, 6],

            # C: Your specified fine-grained range.
            "model__estimator__C": [
                39.0, 39.1, 39.2, 39.3, 39.4, 39.5, 39.6, 39.7, 39.8, 39.9, 40.0, 40.1
            ],

            # Gamma: Complete fine-grained range from 1.55 to 1.61 (inclusive, 0.01 steps).
            "model__estimator__gamma": [
                 1.55, 1.56, 1.57
            ],

            # Epsilon: Complete fine-grained range from 0.50 to 0.60 (inclusive, 0.01 steps).
            "model__estimator__epsilon": [
                0.7, 0.8, 1.0
            ],
        }
    ]
}

"""**4. K-FOLD e GRIDSEARCH**
- K-fold viene utilizzato su X_dev, il nostro Developement Set;
- 4 Fold (64% del totale) -> Diventano il Training Set (del dev);
- 1 Fold (16% del totale) -> Diventano il Validation Set (del dev);
- K-FOLD NON TOCCA ALTRO, INFATTI "X_test_int" (il restante 20%) NON VIENE TOCCATO, TI RICORDO CHE E' IL NOSTRO NUOVO TS (del TRAINING-SET).
"""

# ======================
# K-FOLD CV + GRID SEARCH (SOLO SU X_DEV)
# ======================
# Il KFold divide X_dev in 5 parti:
# - Ogni iterazione: 4 fold (64% del totale) = TRAINING, 1 fold (16% del totale) = VALIDATION
# - X_test_int (20%) rimane completamente isolato
kf = KFold(n_splits=5, shuffle=True, random_state=42)
best_models_dev = {} # Modelli fittati su X_dev con i parametri migliori
cv_results_data = [] # Per salvare i dati grezzi per le Wildcards

print("\n" + "="*60)
print("FASE 1: GridSearchCV su Development Set (80%)")
print(f"Ogni fold CV: ~{X_dev.shape[0]*4//5} samples training, ~{X_dev.shape[0]//5} validation")
print(f"Il 20% Internal Test ({X_test_int.shape[0]} samples) NON viene usato")
print("="*60)

for name, pipe in pipelines.items():
    grid = GridSearchCV(
        estimator=pipe,
        param_grid=param_grids[name],
        cv=kf,
        scoring=mee_scorer,  # Usa MEE invece di MSE
        n_jobs=-1,
        return_train_score=True
    )

    # FITTARE SOLO SU X_DEV!
    grid.fit(X_dev, Y_dev)

    best_models_dev[name] = grid.best_estimator_

    # Salviamo i risultati per l'analisi
    results = grid.cv_results_
    for i in range(len(results["params"])):
        params = results["params"][i]
        mean_train_mee = -results["mean_train_score"][i]
        mean_val_mee = -results["mean_test_score"][i]  # Questo è il CV MEE
        mee_gap = mean_val_mee - mean_train_mee

        cv_results_data.append({
            "Model": name,
            "Params": params,
            "CV_MEE": mean_val_mee,
            "Train_MEE": mean_train_mee,
            "MEE_Gap": mee_gap
        })

# Creiamo DataFrame per facilitare la selezione
df_results = pd.DataFrame(cv_results_data)
df_results = df_results.sort_values(by="CV_MEE")

print("GridSearch Complete.")

# ======================
# SELEZIONE DEL MODELLO MIGLIORE (BASATA SU VALIDATION CV)
# ======================
print("\n" + "="*60)
print("SELEZIONE MODELLO basata su CV MEE (Validation)")
print("="*60)

# SELEZIONE FINALE: Il modello con il CV_MEE più basso
best_model_cv = df_results.iloc[0].to_dict()
print(f"\n✓ MODELLO SELEZIONATO: {best_model_cv['Model']}")
print(f"  Parametri: {best_model_cv['Params']}")
print(f"  CV MEE (Validation): {best_model_cv['CV_MEE']:.6f}")
print(f"  Train MEE: {best_model_cv['Train_MEE']:.6f}")
print(f"  MEE Gap: {best_model_cv['MEE_Gap']:.6f}")

# Per confronto, mostriamo anche altri candidati interessanti
print("\n--- Top 5 modelli per CV MEE ---")
for i, row in df_results.head(5).iterrows():
    print(f"{i+1}. {row['Model']:15s} | CV MEE: {row['CV_MEE']:.6f} | Gap: {row['MEE_Gap']:.6f}")

"""**5. Valutazione su Internal Test Set**

*  Modello ricostruito usando pipeline + iperparametri selezionati;
*  Riaddestrato su tutto il Development Set (X_dev), ATTENZIONE NON HO MAI DETTO SU TUTTO IL TRAINING_SET, ma SOLO SUL DEV COMPLETO.
* Vengono calcolate metriche di errore: MSE, MAE, MEE e R² sul Test Interno;


"""

# ======================
# VALUTAZIONE SU INTERNAL TEST SET (Solo per verifica performance)
# ======================
print("\n" + "="*60)
print("FASE 2: VALUTAZIONE del modello selezionato su Internal Test Set")
print(f"Test su {X_test_int.shape[0]} samples MAI VISTI durante GridSearch/CV")
print("⚠️  NOTA: Il modello è già stato selezionato nella fase precedente")
print("="*60)

# Ricostruiamo il modello selezionato
model_name = best_model_cv["Model"]
params = best_model_cv["Params"]

best_final_model = clone(pipelines[model_name])
best_final_model.set_params(**params)

# Addestriamo su X_DEV
best_final_model.fit(X_dev, Y_dev)

# Prediciamo su X_TEST_INT (Verità vera)
Y_pred_test = best_final_model.predict(X_test_int)
Y_pred_train = best_final_model.predict(X_dev)

# Metriche finali
test_mse = mean_squared_error(Y_test_int, Y_pred_test)
train_mse = mean_squared_error(Y_dev, Y_pred_train)
test_mee = np.mean(np.linalg.norm(Y_test_int - Y_pred_test, axis=1))
test_mae = mean_absolute_error(Y_test_int, Y_pred_test)
r2 = r2_score(Y_test_int, Y_pred_test)
gap = test_mse - train_mse

# Metriche MEE aggiunte
train_mee = mean_euclidean_error(Y_dev, Y_pred_train)
test_mee  = mean_euclidean_error(Y_test_int, Y_pred_test)
mee_gap   = test_mee - train_mee

print(f"\n=== PERFORMANCE SU INTERNAL TEST SET ===")
print(f"Modello: {model_name}")
print(f"Parametri: {params}")
print(f"---")
print(f"MEE (Mean Euclidean Error): {test_mee:.6f}")
print(f"MSE (Mean Squared Error):   {test_mse:.6f}")
print(f"MAE (Mean Absolute Error):  {test_mae:.6f}")
print(f"R² Score:                   {r2:.6f}")
print(f"---")
print(f"Train MSE:                  {train_mse:.6f}")
print(f"Test MSE:                   {test_mse:.6f}")
print(f"MSE Gap (Test - Train):     {gap:.6f}")
print(f"---")
print(f"Train MEE: {train_mee:.6f}")
print(f"Test MEE:  {test_mee:.6f}")
print(f"MEE Gap (Test - Train): {mee_gap:.6f}")# Mostra dettagli predizioni vs target reali (primi 10 samples)
print(f"\n=== DETTAGLIO PREDIZIONI (primi 10 samples del test set) ===")
print(f"{'Sample':<8} | {'Input Features (X)':<60} | {'Predicted (Y_pred)':<35} | {'Actual (Y_true)':<35} | {'Error':<8}")
print("-" * 160)
for i in range(min(10, len(X_test_int))):
    input_str = "[" + ", ".join([f"{x:.3f}" for x in X_test_int[i]]) + "]"
    pred_str = "[" + ", ".join([f"{p:.4f}" for p in Y_pred_test[i]]) + "]"
    true_str = "[" + ", ".join([f"{t:.4f}" for t in Y_test_int[i]]) + "]"
    error = np.linalg.norm(Y_test_int[i] - Y_pred_test[i])
    print(f"{i+1:<8} | {input_str:<60} | {pred_str:<35} | {true_str:<35} | {error:.4f}")

# Salviamo le info del modello finale
best_final_model_info = {
    "model_obj": best_final_model,
    "name": model_name,
    "params": params,
    "test_mee": test_mee,
    "test_mse": test_mse
}

# ======================
# FINAL ANALYSIS: SVR ε-MARGIN (Internal Test) ================================================================================================================
# ======================
def plot_svr_margin(best_model, Y_true, Y_pred, output_idx=0, dataset_name="Internal Test"):
    epsilon = best_model.named_steps["model"].estimator.epsilon

    y_true = Y_true[:, output_idx]
    y_pred = Y_pred[:, output_idx]

    inside = np.abs(y_true - y_pred) <= epsilon
    inside_ratio = inside.mean() * 100

    min_v = min(y_true.min(), y_pred.min())
    max_v = max(y_true.max(), y_pred.max())

    plt.figure(figsize=(6, 5))
    plt.scatter(y_pred, y_true, c=inside, cmap="coolwarm", alpha=0.7)
    plt.plot([min_v, max_v], [min_v, max_v], 'k-', label="y = x")
    plt.plot([min_v, max_v], [min_v + epsilon, max_v + epsilon], 'r--', label="+ε")
    plt.plot([min_v, max_v], [min_v - epsilon, max_v - epsilon], 'r--', label="-ε")

    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(
        f"SVR ε-tube – Output {output_idx}\n"
        f"ε = {epsilon:.3f} | Inside margin: {inside_ratio:.1f}%"
    )
    plt.legend()
    plt.grid()
    plt.show()

for i in range(Y_test_int.shape[1]):
    plot_svr_margin(
        best_model=best_final_model,
        Y_true=Y_test_int,
        Y_pred=Y_pred_test,
        output_idx=i,
        dataset_name="Internal Test"
    )
#======================================================================================================================================================================

# ======================
# LEARNING CURVES (Sul Best Model)
# ======================
from sklearn.model_selection import learning_curve

def plot_learning_curve_final(estimator, X, y, title):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=5, scoring="neg_mean_squared_error",
        train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1
    )

    train_scores_mean = -np.mean(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)

    plt.figure(figsize=(7, 5))
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training MSE")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="CV MSE")
    plt.title(f"Learning Curve: {title}")
    plt.xlabel("Training examples")
    plt.ylabel("MSE")
    plt.legend(loc="best")
    plt.grid()
    plt.show()

# Nota: Passiamo X_dev perché le learning curve usano CV internamente
plot_learning_curve_final(best_final_model_info["model_obj"], X_dev, Y_dev, "Best Model Analysis")


# ======================
# RESIDUALS vs PREDICTED (Internal Test) ================================================================
# ======================

def plot_residuals_vs_predicted(Y_true, Y_pred, output_idx, dataset_name="Internal Test"):
    residuals = Y_true[:, output_idx] - Y_pred[:, output_idx]

    plt.figure(figsize=(6, 4))
    plt.scatter(
        Y_pred[:, output_idx],
        residuals,
        alpha=0.6
    )
    plt.axhline(0, color="r", linestyle="--")
    plt.xlabel("Predicted value")
    plt.ylabel("Residual (True - Predicted)")
    plt.title(
        f"Residuals vs Predicted – Output {output_idx}\n"
        f"{dataset_name}"
    )
    plt.grid()
    plt.show()

# Plot per ciascun output
for i in range(Y_test_int.shape[1]):
    plot_residuals_vs_predicted(
        Y_true=Y_test_int,
        Y_pred=Y_pred_test,
        output_idx=i
    )

"""**6. Retraining finale e BLIND PREDICTION**
- Retraining su TUTTI i dati del TRAINING_SET;
- Blind Prediction sul BLIND_SET;
"""

# ======================
# 8. RETRAINING FINALE E BLIND PREDICTION
# ======================
print("\n" + "="*60)
print("FASE 3: RETRAINING su 100% dei dati (Dev + Internal Test)")
print(f"Utilizzo completo: {X_full.shape[0]} samples")
print("="*60)

# 1. Recuperiamo una copia pulita del modello vincente
final_model = clone(pipelines[best_final_model_info["name"]])
final_model.set_params(**best_final_model_info["params"])

# 2. Addestriamo su TUTTO IL DATASET (Dev + Internal Test)
#    Ora che abbiamo validato il modello, vogliamo massimizzare la conoscenza
final_model.fit(X_full, Y_full)

# 3. Predizione sul file TS blind
Y_blind_pred = final_model.predict(X_blind_test)

# 4. Salvataggio CSV
# Assumiamo che la colonna 0 del TS originale sia l'ID
ids = blind_full.iloc[:, 0].values

# Creiamo array finale: [ID, pred1, pred2, pred3, pred4]
submission_data = np.column_stack((ids, Y_blind_pred))

# Salvataggio
# Formato: ID intero, target float
np.savetxt(
    "submission_linmodels.csv",
    submission_data,
    delimiter=",",
    header="ID,TARGET_x,TARGET_y,TARGET_z,TARGET_w",
    comments='',
    fmt=['%d'] + ['%.9f']*4
)

print("File 'submission_linmodels.csv' generated successfully.")
print("DONE.")

