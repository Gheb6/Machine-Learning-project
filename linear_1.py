# -*- coding: utf-8 -*-
"""LinModels_Corrected.ipynb

Automatically generated by Colab.
"""

# =====================================
# ML-CUP25 - LINEAR MODELS (CORRECTED FLOW)
# HOLD-OUT (80/20) -> K-FOLD CV -> INTERNAL TEST -> BLIND SUBMISSION
# =====================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import KFold, GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, pairwise_distances, make_scorer
from sklearn.base import clone
from prettytable import PrettyTable

# Custom scorer per MEE (Mean Euclidean Error)
def mean_euclidean_error(y_true, y_pred):
    """Calcola la Mean Euclidean Error tra vettori multidimensionali"""
    return np.mean(np.linalg.norm(y_true - y_pred, axis=1))

# Scorer negativo per GridSearchCV (che massimizza, quindi usiamo il negativo)
mee_scorer = make_scorer(mean_euclidean_error, greater_is_better=False)

# ======================
# 1. LOAD DATA & INITIAL SPLIT (HOLD-OUT)
# ======================
TR_PATH = "/content/sample_data/ML-CUP25-TR.csv"
TS_PATH = "/content/sample_data/ML-CUP25-TS.csv"

# Caricamento
df_tr = pd.read_csv(TR_PATH, header=None, comment="#")
df_ts = pd.read_csv(TS_PATH, header=None, comment="#")

# Dati completi
X_full = df_tr.iloc[:, 1:9].values
Y_full = df_tr.iloc[:, 9:13].values
X_blind_test = df_ts.iloc[:, 1:9].values  # Questo serve solo alla fine

# SPLIT FONDAMENTALE: 80% Development (per CV), 20% Internal Test (per check finale)
# ⚠️ IMPORTANTE: X_test_int NON VERRÀ USATO fino alla sezione 6 (valutazione finale)
X_dev, X_test_int, Y_dev, Y_test_int = train_test_split(
    X_full, Y_full, test_size=0.2, random_state=42
)

print(f"Data Split:")
print(f"-> Development Set (GridSearch/CV): {X_dev.shape[0]} samples")
print(f"-> Internal Test Set (Hold-out):    {X_test_int.shape[0]} samples")
print(f"\n⚠️ Il 20% Internal Test Set è BLOCCATO fino alla valutazione finale")

# ======================
# 2. PIPELINES
# ======================
pipelines = {
    "LinearRegression": Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA()),
        ("model", LinearRegression())
    ]),
    "Ridge": Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA()),
        ("model", Ridge())
    ]),
    "Lasso": Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA()),
        ("model", Lasso(max_iter=10000))
    ]),
    "ElasticNet": Pipeline([
        ("scaler", StandardScaler()),
        ("pca", PCA()),
        ("model", ElasticNet(max_iter=10000))
    ])
}

# ======================
# 3. PARAMETER GRIDS
# ======================
param_grids = {
    "LinearRegression": {"pca__n_components": [None, 4, 6, 8]},
    "Ridge": {
        "pca__n_components": [None, 4, 6, 8],
        "model__alpha": [0.01, 0.1, 1, 10, 50, 100, 500, 1000]
    },
    "Lasso": {
        "pca__n_components": [None, 4, 6, 8],
        "model__alpha": [0.001, 0.01, 0.1, 1, 10]
    },
    "ElasticNet": {
        "pca__n_components": [None, 4, 6, 8],
        "model__alpha": [0.001, 0.01, 0.1, 1, 10],
        "model__l1_ratio": [0.1, 0.5, 0.9, 0.99]
    }
}

# ======================
# 4. K-FOLD CV + GRID SEARCH (SOLO SU X_DEV)
# ======================
# Il KFold divide X_dev in 5 parti:
# - Ogni iterazione: 4 fold (64% del totale) = TRAINING, 1 fold (16% del totale) = VALIDATION
# - X_test_int (20%) rimane completamente isolato
kf = KFold(n_splits=5, shuffle=True, random_state=42)
best_models_dev = {} # Modelli fittati su X_dev con i parametri migliori
cv_results_data = [] # Per salvare i dati grezzi per le Wildcards

print("\n" + "="*60)
print("FASE 1: GridSearchCV su Development Set (80%)")
print(f"Ogni fold CV: ~{X_dev.shape[0]*4//5} samples training, ~{X_dev.shape[0]//5} validation")
print(f"Il 20% Internal Test ({X_test_int.shape[0]} samples) NON viene usato")
print("="*60)

for name, pipe in pipelines.items():
    grid = GridSearchCV(
        estimator=pipe,
        param_grid=param_grids[name],
        cv=kf,
        scoring=mee_scorer,  # Usa MEE invece di MSE
        n_jobs=-1,
        return_train_score=True
    )
    
    # FITTARE SOLO SU X_DEV!
    grid.fit(X_dev, Y_dev)
    
    best_models_dev[name] = grid.best_estimator_
    
    # Salviamo i risultati per l'analisi
    results = grid.cv_results_
    for i in range(len(results["params"])):
        params = results["params"][i]
        mean_train_mee = -results["mean_train_score"][i]
        mean_val_mee = -results["mean_test_score"][i]  # Questo è il CV MEE
        mee_gap = mean_val_mee - mean_train_mee
        
        cv_results_data.append({
            "Model": name,
            "Params": params,
            "CV_MEE": mean_val_mee,
            "Train_MEE": mean_train_mee,
            "MEE_Gap": mee_gap
        })

# Creiamo DataFrame per facilitare la selezione
df_results = pd.DataFrame(cv_results_data)
df_results = df_results.sort_values(by="CV_MEE")

print("GridSearch Complete.")

# ======================
# 5. SELEZIONE DEL MODELLO MIGLIORE (BASATA SU VALIDATION CV)
# ======================
print("\n" + "="*60)
print("SELEZIONE MODELLO basata su CV MEE (Validation)")
print("="*60)

# SELEZIONE FINALE: Il modello con il CV_MEE più basso
best_model_cv = df_results.iloc[0].to_dict()
print(f"\n✓ MODELLO SELEZIONATO: {best_model_cv['Model']}")
print(f"  Parametri: {best_model_cv['Params']}")
print(f"  CV MEE (Validation): {best_model_cv['CV_MEE']:.6f}")
print(f"  Train MEE: {best_model_cv['Train_MEE']:.6f}")
print(f"  MEE Gap: {best_model_cv['MEE_Gap']:.6f}")

# Per confronto, mostriamo anche altri candidati interessanti
print("\n--- Top 5 modelli per CV MEE ---")
for i, row in df_results.head(5).iterrows():
    print(f"{i+1}. {row['Model']:15s} | CV MEE: {row['CV_MEE']:.6f} | Gap: {row['MEE_Gap']:.6f}")

# ======================
# 6. VALUTAZIONE SU INTERNAL TEST SET (Solo per verifica performance)
# ======================
print("\n" + "="*60)
print("FASE 2: VALUTAZIONE del modello selezionato su Internal Test Set")
print(f"Test su {X_test_int.shape[0]} samples MAI VISTI durante GridSearch/CV")
print("⚠️  NOTA: Il modello è già stato selezionato nella fase precedente")
print("="*60)

# Ricostruiamo il modello selezionato
model_name = best_model_cv["Model"]
params = best_model_cv["Params"]

best_final_model = clone(pipelines[model_name])
best_final_model.set_params(**params)

# Addestriamo su X_DEV
best_final_model.fit(X_dev, Y_dev)

# Prediciamo su X_TEST_INT (Verità vera)
Y_pred_test = best_final_model.predict(X_test_int)
Y_pred_train = best_final_model.predict(X_dev)

# Metriche finali
test_mse = mean_squared_error(Y_test_int, Y_pred_test)
train_mse = mean_squared_error(Y_dev, Y_pred_train)
test_mee = np.mean(np.linalg.norm(Y_test_int - Y_pred_test, axis=1))
test_mae = mean_absolute_error(Y_test_int, Y_pred_test)
r2 = r2_score(Y_test_int, Y_pred_test)
gap = test_mse - train_mse

print(f"\n=== PERFORMANCE SU INTERNAL TEST SET ===")
print(f"Modello: {model_name}")
print(f"Parametri: {params}")
print(f"---")
print(f"MEE (Mean Euclidean Error): {test_mee:.6f}")
print(f"MSE (Mean Squared Error):   {test_mse:.6f}")
print(f"MAE (Mean Absolute Error):  {test_mae:.6f}")
print(f"R² Score:                   {r2:.6f}")
print(f"---")
print(f"Train MSE:                  {train_mse:.6f}")
print(f"Test MSE:                   {test_mse:.6f}")
print(f"MSE Gap (Test - Train):     {gap:.6f}")

# Salviamo le info del modello finale
best_final_model_info = {
    "model_obj": best_final_model,
    "name": model_name,
    "params": params,
    "test_mee": test_mee,
    "test_mse": test_mse
}

# ======================
# 7. LEARNING CURVES (Sul Best Model)
# ======================
from sklearn.model_selection import learning_curve

def plot_learning_curve_final(estimator, X, y, title):
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=5, scoring="neg_mean_squared_error",
        train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1
    )
    
    train_scores_mean = -np.mean(train_scores, axis=1)
    test_scores_mean = -np.mean(test_scores, axis=1)

    plt.figure(figsize=(7, 5))
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training MSE")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="CV MSE")
    plt.title(f"Learning Curve: {title}")
    plt.xlabel("Training examples")
    plt.ylabel("MSE")
    plt.legend(loc="best")
    plt.grid()
    plt.show()

# Nota: Passiamo X_dev perché le learning curve usano CV internamente
plot_learning_curve_final(best_final_model_info["model_obj"], X_dev, Y_dev, "Best Model Analysis")

# ======================
# 8. RETRAINING FINALE E BLIND PREDICTION
# ======================
print("\n" + "="*60)
print("FASE 3: RETRAINING su 100% dei dati (Dev + Internal Test)")
print(f"Utilizzo completo: {X_full.shape[0]} samples")
print("="*60)

# 1. Recuperiamo una copia pulita del modello vincente
final_model = clone(pipelines[best_final_model_info["name"]])
final_model.set_params(**best_final_model_info["params"])

# 2. Addestriamo su TUTTO IL DATASET (Dev + Internal Test)
#    Ora che abbiamo validato il modello, vogliamo massimizzare la conoscenza
final_model.fit(X_full, Y_full)

# 3. Predizione sul file TS blind
Y_blind_pred = final_model.predict(X_blind_test)

# 4. Salvataggio CSV
# Assumiamo che la colonna 0 del TS originale sia l'ID
ids = df_ts.iloc[:, 0].values

# Creiamo array finale: [ID, pred1, pred2, pred3, pred4]
submission_data = np.column_stack((ids, Y_blind_pred))

# Salvataggio
# Formato: ID intero, target float
np.savetxt(
    "submission_linmodels.csv", 
    submission_data, 
    delimiter=",", 
    header="ID,TARGET_x,TARGET_y,TARGET_z,TARGET_w", 
    comments='', 
    fmt=['%d'] + ['%.9f']*4
)

print("File 'submission_linmodels.csv' generated successfully.")
print("DONE.")