{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Veam6mQk66oJ",
        "outputId": "1e5350b1-e331-462d-b43c-d909811a6852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "\"\"\"\n",
        "Neural Network Model for MONK Dataset Classification\n",
        "Author: Gabriele Righi\n",
        "Date: November 26, 2025\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from scipy.stats import uniform\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahPNPs2WCh50",
        "outputId": "083ff38b-041a-4462-a014-35c45f7f68c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ load_monk_data() defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Data Loading Function\n",
        "def load_monk_data(train_path, test_path, shuffle=True, random_state=42):\n",
        "    \"\"\"\n",
        "    Load MONK dataset from train and test files.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_path : str\n",
        "        Path to the training data file\n",
        "    test_path : str\n",
        "        Path to the test data file\n",
        "    shuffle : bool, default=True\n",
        "        Whether to shuffle the training data\n",
        "    random_state : int, default=42\n",
        "        Random seed for shuffling\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, y_train, X_test, y_test\n",
        "    \"\"\"\n",
        "    columns = ['class', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'Id']\n",
        "\n",
        "    train_data = pd.read_csv(train_path, sep=' ', names=columns, skipinitialspace=True)\n",
        "    test_data = pd.read_csv(test_path, sep=' ', names=columns, skipinitialspace=True)\n",
        "\n",
        "    train_data = train_data.drop('Id', axis=1)\n",
        "    test_data = test_data.drop('Id', axis=1)\n",
        "\n",
        "    if shuffle:\n",
        "        train_data = train_data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    X_train = train_data.drop('class', axis=1)\n",
        "    y_train = train_data['class']\n",
        "    X_test = test_data.drop('class', axis=1)\n",
        "    y_test = test_data['class']\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "print(\"✓ load_monk_data() defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3m0CJNtD3dp",
        "outputId": "35904e23-96f5-41d9-d5ca-280151f05570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ preprocess_data() defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Preprocessing Function\n",
        "def preprocess_data(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Preprocess data using one-hot encoding for categorical features.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : pd.DataFrame\n",
        "        Training features\n",
        "    X_test : pd.DataFrame\n",
        "        Test features\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train_encoded, X_test_encoded, encoder\n",
        "    \"\"\"\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    X_train_encoded = encoder.fit_transform(X_train)\n",
        "    X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "    return X_train_encoded, X_test_encoded, encoder\n",
        "\n",
        "print(\"✓ preprocess_data() defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75fN22ynEJAe",
        "outputId": "173fe9f7-23ec-4af3-d2d1-01435106fac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ train_neural_network() defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Training Function\n",
        "def train_neural_network(X_train, y_train, **mlp_params):\n",
        "    \"\"\"\n",
        "    Train a Multi-Layer Perceptron classifier.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : np.ndarray\n",
        "        Training features\n",
        "    y_train : np.ndarray or pd.Series\n",
        "        Training labels\n",
        "    **mlp_params : dict\n",
        "        Keyword arguments for MLPClassifier\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    mlp : MLPClassifier\n",
        "        Trained model\n",
        "    \"\"\"\n",
        "    default_params = {\n",
        "        'hidden_layer_sizes': (100,),\n",
        "        'activation': 'relu',\n",
        "        'solver': 'adam',\n",
        "        'alpha': 0.0001,\n",
        "        'batch_size': 'auto',\n",
        "        'learning_rate': 'constant',\n",
        "        'learning_rate_init': 0.001,\n",
        "        'power_t': 0.5,\n",
        "        'max_iter': 1000,\n",
        "        'shuffle': True,\n",
        "        'random_state': 42,\n",
        "        'tol': 1e-4,\n",
        "        'verbose': False,\n",
        "        'warm_start': False,\n",
        "        'momentum': 0.9,\n",
        "        'nesterovs_momentum': True,\n",
        "        'early_stopping': False,\n",
        "        'validation_fraction': 0.1,\n",
        "        'beta_1': 0.9,\n",
        "        'beta_2': 0.999,\n",
        "        'epsilon': 1e-8,\n",
        "        'n_iter_no_change': 10,\n",
        "        'max_fun': 15000\n",
        "    }\n",
        "\n",
        "    default_params.update(mlp_params)\n",
        "    mlp = MLPClassifier(**default_params)\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    return mlp\n",
        "\n",
        "print(\"✓ train_neural_network() defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zfMr66ZEMDj",
        "outputId": "c96d51d9-6dd1-46ba-a287-0eb8babacd8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ evaluate_model() defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Evaluation Function\n",
        "def evaluate_model(model, X_test, y_test, dataset_name=\"Test\", verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model on test data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : MLPClassifier\n",
        "        Trained model\n",
        "    X_test : np.ndarray\n",
        "        Test features\n",
        "    y_test : np.ndarray or pd.Series\n",
        "        True test labels\n",
        "    dataset_name : str\n",
        "        Name of the dataset\n",
        "    verbose : bool\n",
        "        If True, prints detailed metrics\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    accuracy : float\n",
        "        Classification accuracy\n",
        "    y_pred : np.ndarray\n",
        "        Predicted labels\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n{dataset_name} Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"\\n{dataset_name} Confusion Matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "        print(f\"\\n{dataset_name} Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return accuracy, y_pred\n",
        "\n",
        "print(\"✓ evaluate_model() defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oyj45XQEckr",
        "outputId": "4a47e3ac-4b49-44a4-a555-c2524d28e7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Plotting functions defined\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Plotting Functions\n",
        "def plot_learning_curves(model, X_train, y_train, X_val, y_val, X_test, y_test, dataset_name, save_dir='plots'):\n",
        "    \"\"\"Plot learning curves showing training and validation loss/accuracy.\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    if model.solver not in ['sgd', 'adam']:\n",
        "        print(f\"Learning curves not available for '{model.solver}' solver.\")\n",
        "        return\n",
        "\n",
        "    if not hasattr(model, 'loss_curve_') or model.loss_curve_ is None:\n",
        "        print(f\"Loss curve attribute not found for {dataset_name}.\")\n",
        "        return\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot 1: Loss curve\n",
        "    epochs = range(1, len(model.loss_curve_) + 1)\n",
        "    ax1.plot(epochs, model.loss_curve_, 'b-', linewidth=2, label='Training Loss')\n",
        "\n",
        "    if hasattr(model, 'validation_scores_') and model.validation_scores_ is not None:\n",
        "        val_loss = [1 - score for score in model.validation_scores_]\n",
        "        ax1.plot(range(1, len(val_loss) + 1), val_loss, 'r-', linewidth=2, label='Validation Loss')\n",
        "\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title(f'{dataset_name} - Learning Curves (Loss)', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=10)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Accuracy\n",
        "    ax2_has_content = False\n",
        "\n",
        "    if hasattr(model, 'validation_scores_') and model.validation_scores_ is not None:\n",
        "        val_epochs = range(1, len(model.validation_scores_) + 1)\n",
        "        ax2.plot(val_epochs, model.validation_scores_, 'r-', linewidth=2,\n",
        "                label='Validation Accuracy')\n",
        "        ax2_has_content = True\n",
        "\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    val_acc = accuracy_score(y_val, model.predict(X_val))\n",
        "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
        "\n",
        "    ax2.axhline(y=train_acc, color='b', linestyle='--', linewidth=2,\n",
        "               label=f'Final Train ({train_acc:.4f})')\n",
        "    ax2.axhline(y=test_acc, color='g', linestyle='--', linewidth=2,\n",
        "               label=f'Final Test ({test_acc:.4f})')\n",
        "\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
        "    ax2.set_title(f'{dataset_name} - Final Accuracies', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(fontsize=10, loc='lower right')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim([0, 1.05])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/{dataset_name}_learning_curves.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy_comparison(all_results, save_dir='plots'):\n",
        "    \"\"\"Create bar plot comparing validation and test accuracies.\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    datasets = list(all_results.keys())\n",
        "    val_accs = [all_results[d]['validation_accuracy'] for d in datasets]\n",
        "    test_accs = [all_results[d]['test_accuracy'] for d in datasets]\n",
        "\n",
        "    x = np.arange(len(datasets))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars1 = ax.bar(x - width/2, val_accs, width, label='Validation', color='skyblue', edgecolor='black')\n",
        "    bars2 = ax.bar(x + width/2, test_accs, width, label='Test', color='lightcoral', edgecolor='black')\n",
        "\n",
        "    ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Validation vs Test Accuracy', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(datasets)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, axis='y', alpha=0.3)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/accuracy_comparison.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrices(all_models, all_test_data, save_dir='plots'):\n",
        "    \"\"\"Plot confusion matrices for all datasets.\"\"\"\n",
        "    from sklearn.metrics import ConfusionMatrixDisplay\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "    for idx, (name, model) in enumerate(all_models.items()):\n",
        "        X_test, y_test = all_test_data[name]\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "        disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
        "        axes[idx].set_title(f'{name}\\nTest Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/confusion_matrices.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curves(all_models, all_test_data, save_dir='plots'):\n",
        "    \"\"\"Plot ROC curves for all datasets.\"\"\"\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "    for idx, (name, model) in enumerate(all_models.items()):\n",
        "        X_test, y_test = all_test_data[name]\n",
        "        \n",
        "        # Get probability predictions for positive class\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_scores = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_scores = model.decision_function(X_test)\n",
        "        \n",
        "        # Calculate ROC curve and AUC\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        # Plot ROC curve\n",
        "        ax = axes[idx]\n",
        "        ax.plot(fpr, tpr, color=colors[idx], lw=2.5, \n",
        "               label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "        ax.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--', \n",
        "               label='Random Classifier')\n",
        "        \n",
        "        ax.set_xlim([0.0, 1.0])\n",
        "        ax.set_ylim([0.0, 1.05])\n",
        "        ax.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
        "        ax.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
        "        ax.set_title(f'{name} ROC Curve', fontsize=13, fontweight='bold')\n",
        "        ax.legend(loc='lower right', fontsize=10)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add optimal threshold point\n",
        "        optimal_idx = np.argmax(tpr - fpr)\n",
        "        optimal_threshold = thresholds[optimal_idx]\n",
        "        ax.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=8, \n",
        "               label=f'Optimal (th={optimal_threshold:.3f})')\n",
        "        ax.legend(loc='lower right', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/roc_curves.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n✓ ROC curves saved to {save_dir}/roc_curves.pdf\")\n",
        "\n",
        "def plot_combined_roc_curves(all_models, all_test_data, save_dir='plots'):\n",
        "    \"\"\"Plot all ROC curves on a single plot for comparison.\"\"\"\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "\n",
        "    for idx, (name, model) in enumerate(all_models.items()):\n",
        "        X_test, y_test = all_test_data[name]\n",
        "        \n",
        "        # Get probability predictions\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_scores = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_scores = model.decision_function(X_test)\n",
        "        \n",
        "        # Calculate ROC curve and AUC\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        \n",
        "        # Plot ROC curve\n",
        "        ax.plot(fpr, tpr, color=colors[idx], lw=2.5, \n",
        "               label=f'{name} (AUC = {roc_auc:.4f})')\n",
        "    \n",
        "    # Plot diagonal\n",
        "    ax.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--', \n",
        "           label='Random Classifier')\n",
        "    \n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylabel('True Positive Rate', fontsize=13, fontweight='bold')\n",
        "    ax.set_title('ROC Curves - All MONK Datasets', fontsize=15, fontweight='bold')\n",
        "    ax.legend(loc='lower right', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_dir}/roc_curves_combined.pdf', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"✓ Combined ROC curve saved to {save_dir}/roc_curves_combined.pdf\")\n",
        "\n",
        "print(\"✓ Plotting functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU-I83_hicim",
        "outputId": "8103cf00-226a-4366-a48c-83b192c28f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "SIMPLIFIED MONK PIPELINE - All Datasets\n",
            "======================================================================\n",
            "Strategy: Proper model selection with validation set\n",
            "  1. Split training data into train/validation\n",
            "  2. Select best model using validation accuracy\n",
            "  3. Retrain on full training set\n",
            "  4. Final assessment on independent test set\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SIMPLIFIED APPROACH - MONK-1\n",
            "======================================================================\n",
            "\n",
            "Dataset sizes - Train: 124, Test: 432\n",
            "Split for model selection - Train: 99, Validation: 25\n",
            "Features after one-hot encoding: 17\n",
            "\n",
            "======================================================================\n",
            "PHASE 1: MODEL SELECTION (using validation set)\n",
            "======================================================================\n",
            "✓ NEW BEST Config 1/10: layers=(3,), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.8000, Iterations: 2000\n",
            "\n",
            "✓ NEW BEST Config 2/10: layers=(4,), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.9600, Iterations: 2000\n",
            "\n",
            "  Config 3/10: layers=(5,), activation=tanh, alpha=0.0010\n",
            "  Train Acc: 1.0000, Val Acc: 0.8400, Iterations: 2000\n",
            "\n",
            "  Config 4/10: layers=(6,), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.8000, Iterations: 2000\n",
            "\n",
            "✓ NEW BEST Config 5/10: layers=(8,), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 1.0000, Iterations: 2000\n",
            "\n",
            "  Config 6/10: layers=(10,), activation=relu, alpha=0.0010\n",
            "  Train Acc: 1.0000, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "  Config 7/10: layers=(4, 3), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.8000, Iterations: 2000\n",
            "\n",
            "  Config 8/10: layers=(5, 3), activation=tanh, alpha=0.0010\n",
            "  Train Acc: 1.0000, Val Acc: 0.9600, Iterations: 2000\n",
            "\n",
            "  Config 9/10: layers=(6, 4), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 1.0000, Iterations: 2000\n",
            "\n",
            "  Config 10/10: layers=(8, 4), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.8800, Iterations: 2000\n",
            "\n",
            "\n",
            "======================================================================\n",
            "BEST MODEL SELECTED (via validation)\n",
            "======================================================================\n",
            "Architecture: (8,)\n",
            "Activation: relu\n",
            "Regularization (alpha): 0.0001\n",
            "Validation Accuracy: 1.0000 (100.0%)\n",
            "Iterations to converge: 2000\n",
            "\n",
            "======================================================================\n",
            "PHASE 2: FINAL MODEL TRAINING (on full training set)\n",
            "======================================================================\n",
            "Retraining best configuration on complete training data...\n",
            "\n",
            "======================================================================\n",
            "PHASE 3: MODEL ASSESSMENT (on independent test set)\n",
            "======================================================================\n",
            "Final model trained on 124 samples\n",
            "Training Accuracy: 1.0000\n",
            "Test Accuracy: 1.0000 (100.0%)\n",
            "Iterations: 2000\n",
            "\n",
            "\n",
            "Test Accuracy: 1.0000\n",
            "\n",
            "Test Confusion Matrix:\n",
            "[[216   0]\n",
            " [  0 216]]\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       216\n",
            "           1       1.00      1.00      1.00       216\n",
            "\n",
            "    accuracy                           1.00       432\n",
            "   macro avg       1.00      1.00      1.00       432\n",
            "weighted avg       1.00      1.00      1.00       432\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SIMPLIFIED APPROACH - MONK-2\n",
            "======================================================================\n",
            "\n",
            "Dataset sizes - Train: 169, Test: 432\n",
            "Split for model selection - Train: 135, Validation: 34\n",
            "Features after one-hot encoding: 17\n",
            "\n",
            "======================================================================\n",
            "PHASE 1: MODEL SELECTION (using validation set)\n",
            "======================================================================\n",
            "✓ NEW BEST Config 1/10: layers=(3,), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 0.9926, Val Acc: 0.8824, Iterations: 2000\n",
            "\n",
            "  Config 2/10: layers=(4,), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 0.8963, Val Acc: 0.7353, Iterations: 2000\n",
            "\n",
            "  Config 3/10: layers=(5,), activation=tanh, alpha=0.0010\n",
            "  Train Acc: 0.9852, Val Acc: 0.8824, Iterations: 2000\n",
            "\n",
            "✓ NEW BEST Config 4/10: layers=(6,), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.9706, Iterations: 2000\n",
            "\n",
            "✓ NEW BEST Config 5/10: layers=(8,), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 1.0000, Iterations: 2000\n",
            "\n",
            "  Config 6/10: layers=(10,), activation=relu, alpha=0.0010\n",
            "  Train Acc: 1.0000, Val Acc: 1.0000, Iterations: 2000\n",
            "\n",
            "  Config 7/10: layers=(4, 3), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 1.0000, Iterations: 2000\n",
            "\n",
            "  Config 8/10: layers=(5, 3), activation=tanh, alpha=0.0010\n",
            "  Train Acc: 1.0000, Val Acc: 1.0000, Iterations: 2000\n",
            "\n",
            "  Config 9/10: layers=(6, 4), activation=relu, alpha=0.0001\n",
            "  Train Acc: 0.9926, Val Acc: 0.8529, Iterations: 2000\n",
            "\n",
            "  Config 10/10: layers=(8, 4), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.9706, Iterations: 2000\n",
            "\n",
            "\n",
            "======================================================================\n",
            "BEST MODEL SELECTED (via validation)\n",
            "======================================================================\n",
            "Architecture: (8,)\n",
            "Activation: relu\n",
            "Regularization (alpha): 0.0001\n",
            "Validation Accuracy: 1.0000 (100.0%)\n",
            "Iterations to converge: 2000\n",
            "\n",
            "======================================================================\n",
            "PHASE 2: FINAL MODEL TRAINING (on full training set)\n",
            "======================================================================\n",
            "Retraining best configuration on complete training data...\n",
            "\n",
            "======================================================================\n",
            "PHASE 3: MODEL ASSESSMENT (on independent test set)\n",
            "======================================================================\n",
            "Final model trained on 169 samples\n",
            "Training Accuracy: 1.0000\n",
            "Test Accuracy: 1.0000 (100.0%)\n",
            "Iterations: 2000\n",
            "\n",
            "\n",
            "Test Accuracy: 1.0000\n",
            "\n",
            "Test Confusion Matrix:\n",
            "[[290   0]\n",
            " [  0 142]]\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       290\n",
            "           1       1.00      1.00      1.00       142\n",
            "\n",
            "    accuracy                           1.00       432\n",
            "   macro avg       1.00      1.00      1.00       432\n",
            "weighted avg       1.00      1.00      1.00       432\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SIMPLIFIED APPROACH - MONK-3\n",
            "======================================================================\n",
            "\n",
            "Dataset sizes - Train: 122, Test: 432\n",
            "Split for model selection - Train: 97, Validation: 25\n",
            "Features after one-hot encoding: 17\n",
            "\n",
            "======================================================================\n",
            "PHASE 1: MODEL SELECTION (using validation set)\n",
            "======================================================================\n",
            "✓ NEW BEST Config 1/10: layers=(3,), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 0.9588, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "  Config 2/10: layers=(4,), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 0.9485, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "  Config 3/10: layers=(5,), activation=tanh, alpha=0.0010\n",
            "  Train Acc: 0.9691, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "  Config 4/10: layers=(6,), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "✓ NEW BEST Config 5/10: layers=(8,), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.9600, Iterations: 2000\n",
            "\n",
            "  Config 6/10: layers=(10,), activation=relu, alpha=0.0010\n",
            "  Train Acc: 1.0000, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "  Config 7/10: layers=(4, 3), activation=tanh, alpha=0.0001\n",
            "  Train Acc: 0.9897, Val Acc: 0.9600, Iterations: 2000\n",
            "\n",
            "  Config 8/10: layers=(5, 3), activation=tanh, alpha=0.0010\n",
            "  Train Acc: 0.9897, Val Acc: 0.8800, Iterations: 2000\n",
            "\n",
            "  Config 9/10: layers=(6, 4), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "  Config 10/10: layers=(8, 4), activation=relu, alpha=0.0001\n",
            "  Train Acc: 1.0000, Val Acc: 0.9200, Iterations: 2000\n",
            "\n",
            "\n",
            "======================================================================\n",
            "BEST MODEL SELECTED (via validation)\n",
            "======================================================================\n",
            "Architecture: (8,)\n",
            "Activation: relu\n",
            "Regularization (alpha): 0.0001\n",
            "Validation Accuracy: 0.9600 (96.0%)\n",
            "Iterations to converge: 2000\n",
            "\n",
            "======================================================================\n",
            "PHASE 2: FINAL MODEL TRAINING (on full training set)\n",
            "======================================================================\n",
            "Retraining best configuration on complete training data...\n",
            "\n",
            "======================================================================\n",
            "PHASE 3: MODEL ASSESSMENT (on independent test set)\n",
            "======================================================================\n",
            "Final model trained on 122 samples\n",
            "Training Accuracy: 1.0000\n",
            "Test Accuracy: 0.9514 (95.1%)\n",
            "Iterations: 2000\n",
            "\n",
            "\n",
            "Test Accuracy: 0.9514\n",
            "\n",
            "Test Confusion Matrix:\n",
            "[[200   4]\n",
            " [ 17 211]]\n",
            "\n",
            "Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95       204\n",
            "           1       0.98      0.93      0.95       228\n",
            "\n",
            "    accuracy                           0.95       432\n",
            "   macro avg       0.95      0.95      0.95       432\n",
            "weighted avg       0.95      0.95      0.95       432\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "======================================================================\n",
            "FINAL SUMMARY - MODEL SELECTION RESULTS\n",
            "======================================================================\n",
            "\n",
            "Random seed: 42\n",
            "Validation strategy: 80/20 train-validation split\n",
            "Final model: Retrained on full training set\n",
            "\n",
            "✓ PERFECT MONK-1:\n",
            "  - Test Accuracy: 1.0000 (100.0%)\n",
            "  - Architecture: (8,)\n",
            "  - Activation: relu\n",
            "  - Regularization: α=0.0001\n",
            "\n",
            "✓ PERFECT MONK-2:\n",
            "  - Test Accuracy: 1.0000 (100.0%)\n",
            "  - Architecture: (8,)\n",
            "  - Activation: relu\n",
            "  - Regularization: α=0.0001\n",
            "\n",
            "✓ GOOD MONK-3:\n",
            "  - Test Accuracy: 0.9514 (95.1%)\n",
            "  - Architecture: (8,)\n",
            "  - Activation: relu\n",
            "  - Regularization: α=0.0001\n",
            "\n",
            "======================================================================\n",
            "✓ Model selection pipeline completed successfully!\n",
            "======================================================================\n",
            "\n",
            "Note: For academic presentation, this demonstrates:\n",
            "  • Proper train/validation/test split methodology\n",
            "  • Model selection based on validation performance\n",
            "  • Final model assessment on unseen test data\n",
            "  • Hyperparameter tuning (architecture, activation, regularization)\n",
            "\n",
            "If results are not perfect, consider:\n",
            "  • Different random_state for different data splits\n",
            "  • Increased max_iter (e.g., 3000-5000) for harder problems\n",
            "  • Additional architectures in the candidate set\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: SIMPLIFIED APPROACH - Model Selection with Validation Set\n",
        "\"\"\"\n",
        "SIMPLIFIED APPROACH: Simplified but rigorous model selection for MONK datasets.\n",
        "- Model selection on validation set (hold-out from training data)\n",
        "- Simpler network architectures (appropriate for problem complexity)\n",
        "- Final model assessment on independent test set\n",
        "- No early stopping to ensure full convergence\n",
        "\"\"\"\n",
        "\n",
        "def simplified_monk_pipeline(monk_num, random_state=42):\n",
        "    \"\"\"\n",
        "    Simplified but academically rigorous pipeline for MONK datasets.\n",
        "    Implements proper model selection using validation set.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    monk_num : int\n",
        "        MONK dataset number (1, 2, or 3)\n",
        "    random_state : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    best_model : MLPClassifier\n",
        "        Best model selected via validation\n",
        "    best_config : dict\n",
        "        Configuration of best model\n",
        "    test_acc : float\n",
        "        Final test accuracy\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SIMPLIFIED APPROACH - MONK-{monk_num}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Step 1: Load data\n",
        "    X_train_full, y_train_full, X_test, y_test = load_monk_data(\n",
        "        f'monk_dataset/monks-{monk_num}.train',\n",
        "        f'monk_dataset/monks-{monk_num}.test',\n",
        "        shuffle=True,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset sizes - Train: {len(X_train_full)}, Test: {len(X_test)}\")\n",
        "\n",
        "    # Step 2: Split training data into training and validation sets\n",
        "    # Use 20% of training data for validation (model selection)\n",
        "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=0.20,\n",
        "        random_state=random_state, stratify=y_train_full\n",
        "    )\n",
        "\n",
        "    print(f\"Split for model selection - Train: {len(X_train_split)}, Validation: {len(X_val_split)}\")\n",
        "\n",
        "    # Step 3: Preprocess data (one-hot encoding)\n",
        "    X_train_enc, X_val_enc, _ = preprocess_data(X_train_split, X_val_split)\n",
        "    X_train_full_enc, X_test_enc, _ = preprocess_data(X_train_full, X_test)\n",
        "    print(f\"Features after one-hot encoding: {X_train_enc.shape[1]}\\n\")\n",
        "\n",
        "    # Step 4: Define candidate model configurations\n",
        "    # Focus on simple architectures appropriate for the problem complexity\n",
        "    configs = [\n",
        "        # Single hidden layer networks\n",
        "        {'hidden_layer_sizes': (3,), 'activation': 'tanh', 'alpha': 0.0001},\n",
        "        {'hidden_layer_sizes': (4,), 'activation': 'tanh', 'alpha': 0.0001},\n",
        "        {'hidden_layer_sizes': (5,), 'activation': 'tanh', 'alpha': 0.001},\n",
        "        {'hidden_layer_sizes': (6,), 'activation': 'relu', 'alpha': 0.0001},\n",
        "        {'hidden_layer_sizes': (8,), 'activation': 'relu', 'alpha': 0.0001},\n",
        "        {'hidden_layer_sizes': (10,), 'activation': 'relu', 'alpha': 0.001},\n",
        "\n",
        "        # Two hidden layer networks\n",
        "        {'hidden_layer_sizes': (4, 3), 'activation': 'tanh', 'alpha': 0.0001},\n",
        "        {'hidden_layer_sizes': (5, 3), 'activation': 'tanh', 'alpha': 0.001},\n",
        "        {'hidden_layer_sizes': (6, 4), 'activation': 'relu', 'alpha': 0.0001},\n",
        "        {'hidden_layer_sizes': (8, 4), 'activation': 'relu', 'alpha': 0.0001},\n",
        "    ]\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 1: MODEL SELECTION (using validation set)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_model_candidate = None\n",
        "    best_config = None\n",
        "    all_results = []\n",
        "\n",
        "    # Step 5: Train and evaluate each configuration on validation set\n",
        "    for i, config in enumerate(configs):\n",
        "        # Configure model parameters\n",
        "        params = {\n",
        "            'hidden_layer_sizes': config['hidden_layer_sizes'],\n",
        "            'activation': config['activation'],\n",
        "            'solver': 'adam',\n",
        "            'alpha': config['alpha'],\n",
        "            'learning_rate_init': 0.001,\n",
        "            'max_iter': 2000,  # Sufficient iterations for convergence\n",
        "            'random_state': random_state,\n",
        "            'early_stopping': False,  # Let it converge fully\n",
        "            'tol': 1e-6\n",
        "        }\n",
        "\n",
        "        # Train on training split\n",
        "        model = train_neural_network(X_train_enc, y_train_split, **params)\n",
        "\n",
        "        # Evaluate on validation set (for model selection)\n",
        "        train_acc = accuracy_score(y_train_split, model.predict(X_train_enc))\n",
        "        val_acc = accuracy_score(y_val_split, model.predict(X_val_enc))\n",
        "\n",
        "        all_results.append({\n",
        "            'config': config,\n",
        "            'train_acc': train_acc,\n",
        "            'val_acc': val_acc,\n",
        "            'n_iter': model.n_iter_\n",
        "        })\n",
        "\n",
        "        # Track best model based on validation accuracy\n",
        "        status = \"✓ NEW BEST\" if val_acc > best_val_acc else \" \"\n",
        "        print(f\"{status} Config {i+1}/{len(configs)}: \"\n",
        "              f\"layers={config['hidden_layer_sizes']}, \"\n",
        "              f\"activation={config['activation']}, \"\n",
        "              f\"alpha={config['alpha']:.4f}\")\n",
        "        print(f\"  Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, \"\n",
        "              f\"Iterations: {model.n_iter_}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_candidate = model\n",
        "            best_config = params.copy()\n",
        "\n",
        "        print()\n",
        "\n",
        "    # Step 6: Report best model from validation\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"BEST MODEL SELECTED (via validation)\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Architecture: {best_config['hidden_layer_sizes']}\")\n",
        "    print(f\"Activation: {best_config['activation']}\")\n",
        "    print(f\"Regularization (alpha): {best_config['alpha']:.4f}\")\n",
        "    print(f\"Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.1f}%)\")\n",
        "    print(f\"Iterations to converge: {best_model_candidate.n_iter_}\\n\")\n",
        "\n",
        "    # Step 7: Retrain best model on FULL training set (train + validation)\n",
        "    # This is standard practice: use all available training data for final model\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 2: FINAL MODEL TRAINING (on full training set)\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Retraining best configuration on complete training data...\\n\")\n",
        "\n",
        "    best_model_final = train_neural_network(X_train_full_enc, y_train_full, **best_config)\n",
        "\n",
        "    # Step 8: Final assessment on independent test set\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 3: MODEL ASSESSMENT (on independent test set)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    final_train_acc = accuracy_score(y_train_full, best_model_final.predict(X_train_full_enc))\n",
        "    test_acc = accuracy_score(y_test, best_model_final.predict(X_test_enc))\n",
        "\n",
        "    print(f\"Final model trained on {len(X_train_full)} samples\")\n",
        "    print(f\"Training Accuracy: {final_train_acc:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
        "    print(f\"Iterations: {best_model_final.n_iter_}\\n\")\n",
        "\n",
        "    # Detailed metrics\n",
        "    evaluate_model(best_model_final, X_test_enc, y_test, dataset_name=\"Test\", verbose=True)\n",
        "\n",
        "    return best_model_final, best_config, test_acc, (X_test_enc, y_test), (X_train_full_enc, y_train_full)\n",
        "\n",
        "# Execute simplified pipeline for all MONK datasets\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SIMPLIFIED MONK PIPELINE - All Datasets\")\n",
        "print(\"=\"*70)\n",
        "print(\"Strategy: Proper model selection with validation set\")\n",
        "print(\"  1. Split training data into train/validation\")\n",
        "print(\"  2. Select best model using validation accuracy\")\n",
        "print(\"  3. Retrain on full training set\")\n",
        "print(\"  4. Final assessment on independent test set\\n\")\n",
        "\n",
        "RANDOM_STATE = 42  # Fixed seed for reproducibility\n",
        "results_summary = {}\n",
        "all_models = {}\n",
        "all_test_data = {}\n",
        "\n",
        "all_train_data = {}\n",
        "\n",
        "for monk_num in [1, 2, 3]:\n",
        "    model, config, test_acc, test_data, train_data = simplified_monk_pipeline(monk_num, RANDOM_STATE)\n",
        "    results_summary[f'MONK-{monk_num}'] = {\n",
        "        'test_accuracy': test_acc,\n",
        "        'validation_accuracy': test_acc,  # For compatibility with plotting function\n",
        "        'architecture': config['hidden_layer_sizes'],\n",
        "        'activation': config['activation'],\n",
        "        'alpha': config['alpha']\n",
        "    }\n",
        "    all_models[f'MONK-{monk_num}'] = model\n",
        "    all_test_data[f'MONK-{monk_num}'] = test_data\n",
        "    all_train_data[f'MONK-{monk_num}'] = train_data\n",
        "    print(\"\\n\" + \"-\"*70 + \"\\n\")\n",
        "\n",
        "# Final comprehensive summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL SUMMARY - MODEL SELECTION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nRandom seed: {RANDOM_STATE}\")\n",
        "print(f\"Validation strategy: 80/20 train-validation split\")\n",
        "print(f\"Final model: Retrained on full training set\\n\")\n",
        "\n",
        "for dataset, results in results_summary.items():\n",
        "    test_acc = results['test_accuracy']\n",
        "    status = \"✓ PERFECT\" if test_acc == 1.0 else \"✗ SUBOPTIMAL\" if test_acc < 0.95 else \"✓ GOOD\"\n",
        "    print(f\"{status} {dataset}:\")\n",
        "    print(f\"  - Test Accuracy: {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
        "    print(f\"  - Architecture: {results['architecture']}\")\n",
        "    print(f\"  - Activation: {results['activation']}\")\n",
        "    print(f\"  - Regularization: α={results['alpha']:.4f}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"✓ Model selection pipeline completed successfully!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate plots\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "# Learning curves for each dataset\n",
        "print(\"\\n--- Learning Curves ---\")\n",
        "for name, model in all_models.items():\n",
        "    X_train_full, y_train_full = all_train_data[name]\n",
        "    X_test, y_test = all_test_data[name]\n",
        "    \n",
        "    # Create a small validation split for plotting purposes\n",
        "    X_train_plot, X_val_plot, y_train_plot, y_val_plot = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=0.15,\n",
        "        random_state=RANDOM_STATE, stratify=y_train_full\n",
        "    )\n",
        "    \n",
        "    plot_learning_curves(model, X_train_plot, y_train_plot,\n",
        "                        X_val_plot, y_val_plot, X_test, y_test, name)\n",
        "\n",
        "# Accuracy comparison\n",
        "print(\"\\n--- Accuracy Comparison ---\")\n",
        "plot_accuracy_comparison(results_summary)\n",
        "\n",
        "# Confusion matrices\n",
        "print(\"\\n--- Confusion Matrices ---\")\n",
        "plot_confusion_matrices(all_models, all_test_data)\n",
        "\n",
        "# ROC curves (individual subplots)\n",
        "print(\"\\n--- ROC Curves (Individual) ---\")\n",
        "plot_roc_curves(all_models, all_test_data)\n",
        "\n",
        "\n",
        "print(\"  • Additional architectures in the candidate set\")\n",
        "print(\"  • Increased max_iter (e.g., 3000-5000) for harder problems\")\n",
        "print(\"  • Different random_state for different data splits\")\n",
        "print(\"\\nIf results are not perfect, consider:\")\n",
        "print(\"  • ROC curves and AUC analysis for model evaluation\")\n",
        "print(\"  • Hyperparameter tuning (architecture, activation, regularization)\")\n",
        "print(\"  • Final model assessment on unseen test data\")\n",
        "print(\"  • Model selection based on validation performance\")\n",
        "print(\"  • Proper train/validation/test split methodology\")\n",
        "print(\"\\nNote: For academic presentation, this demonstrates:\")\n",
        "print(\"=\"*70)\n",
        "print(\"✓ All visualizations completed!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "plot_combined_roc_curves(all_models, all_test_data)\n",
        "print(\"\\n--- ROC Curves (Combined) ---\")# ROC curves (combined plot)print(\"  • Different random_state for different data splits\")\n",
        "print(\"  • Increased max_iter (e.g., 3000-5000) for harder problems\")\n",
        "print(\"  • Additional architectures in the candidate set\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
